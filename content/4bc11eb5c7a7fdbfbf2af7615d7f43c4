<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="generator" content="HTML Tidy, see www.w3.org" />
<title>Topics in High-Performance Messaging</title>
<meta name="GENERATOR" content="Modular DocBook HTML Stylesheet Version 1.79" />
<link rel="STYLESHEET" type="text/css" href="docbook.css" />
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
</head>
<body class="ARTICLE" bgcolor="#FFFFFF" text="#000000" link="#0000FF" vlink="#840084"
alink="#0000FF">
<div class="ARTICLE">
<div class="TITLEPAGE">
<h1 class="TITLE"><a id="AEN2" name="AEN2">Topics in High-Performance Messaging</a></h1>

<h3 class="AUTHOR"><a id="AEN4" name="AEN4">Robert A. Van Valzah</a></h3>

<h3 class="AUTHOR"><a id="AEN7" name="AEN7">Todd L. Montgomery</a></h3>

<h3 class="AUTHOR"><a id="AEN10" name="AEN10">Eric Bowden</a></h3>

<p class="COPYRIGHT">Copyright &copy; 2004 - 2010 29West, Inc.</p>

<p class="PUBDATE">December 2009<br />
</p>

<div class="DOCFORAMTNAVI">[ <a href="./index.html">Split HTML</a> / <a
href="thpm.html">Single HTML</a> / <a href="thpm.pdf">Single PDF</a> ]</div>

<hr />
</div>

<div class="TOC">
<dl>
<dt><b>Table of Contents</b></dt>

<dt>1. <a href="#INTRODUCTION">Introduction</a></dt>

<dt>2. <a href="#TCP-LATENCY">TCP Latency</a></dt>

<dt>3. <a href="#GROUP-RATE-CONTROL">Group Rate Control</a></dt>

<dt>4. <a href="#ETHERNET-FLOW-CONTROL">Ethernet Flow Control</a></dt>

<dt>5. <a href="#PACKET-LOSS-MYTHS">Packet Loss Myths</a></dt>

<dt>6. <a href="#MONITORING-MESSAGING-SYSTEMS">Monitoring Messaging Systems</a></dt>

<dt>7. <a href="#UDP-BUFFERING-BACKGROUND">UDP Buffering Background</a></dt>

<dt>8. <a href="#UDP-BUFFER-SIZING">UDP Buffer Sizing</a></dt>

<dt>9. <a href="#MULTICAST-LOOPBACK">Multicast Loopback</a></dt>

<dt>10. <a href="#SENDING-MULTICAST-MULTIPLE-INTERFACES">Sending Multicast on Multiple
Interfaces</a></dt>

<dt>11. <a href="#TTL-0-KEEP-MULTICAST-LOCAL">TTL=0 to Keep Multicast Local</a></dt>

<dt>12. <a href="#TTL-1-AND-CISCO-CPU-USAGE">TTL=1 and Cisco CPU Usage</a></dt>

<dt>13. <a href="#INTERMITTENT-NETWORK-MULTICAST-LOSS">Intermittent Network Multicast
Loss</a></dt>

<dt>14. <a href="#MULTICAST-ADDRESS-ASSIGNMENT">Multicast Address Assignment</a></dt>

<dt>15. <a href="#MULTICAST-RETRANSMISSIONS">Multicast Retransmissions</a></dt>

<dt>16. <a href="#MESSAGING-LATENCY-BUDGET">Messaging Latency Budget</a></dt>

<dt>17. <a href="#LATENCY-SOURCES">Sources of Latency</a></dt>

<dt>18. <a href="#LATENCY-INTERRUPT-COALESCING">Latency from Interrupt
Coalescing</a></dt>

<dt>19. <a href="#LATENCY-MEASUREMENT-OVERVIEW">Latency Measurement Overview</a></dt>

<dt>20. <a href="#MEASURING-CPU-SCHEDULING-LATENCY">Measuring CPU Scheduling
Latency</a></dt>

<dt>21. <a href="#MEASURING-CPU-CONTENTION-LATENCY">Measuring CPU Contention
Latency</a></dt>
</dl>
</div>

<blockquote class="ABSTRACT">
<div class="ABSTRACT"><a id="AEN17" name="AEN17"></a>
<p><b>Abstract</b></p>

<p>We have been working together in the field of high-performance messaging for many
years. We have seen many deployed messaging systems that worked well and many that
didn't. The background information needed for successful deployment isn't widely
available; most of what we know we had to learn in the school of hard knocks. In hopes of
saving others a knock or two, we have tried to collect here background information and
commentary on some of the issues involved in successful deployments. This information is
organized as a series of topics around which there seems to be confusion or uncertainty.
Please contact us through <a href="http://www.29West.Com/" target="_top">29West</a> if
you have questions or comments.</p>
</div>
</blockquote>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="INTRODUCTION" name="INTRODUCTION">1. Introduction</a></h2>

<p>In the field of high-performance messaging systems, performance tends to be the
dominant factor in making design decisions. In this context, "performance" can indicate
high message rates, high payload data transfer rates, low latency, high scalability, high
efficiency, or all of the above. Such factors tend to be important in applications like
financial market data, satellite, telemetry, and military command &amp; control.</p>

<p>Successful deployment of high-performance messaging systems requires
cross-disciplinary knowledge. System-level issues must be considered including network,
OS, and host hardware issues.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="TCP-LATENCY" name="TCP-LATENCY">2. TCP Latency</a></h2>

<p>TCP can be used for latency-sensitive applications, but several factors must be
considered when choosing it over other protocols that may be better suited to
latency-sensitive applications.</p>

<p>TCP provides low-latency delivery only if:</p>

<ul>
<li>
<p>All receivers can always keep up with the sender</p>
</li>

<li>
<p>The network is never congested</p>
</li>
</ul>

<p>This essentially boils down to "TCP is OK for latency-sensitive applications if
nothing ever goes wrong."</p>

<p>A little historical perspective helps in understanding TCP. It's easy for one user of
a network to think about TCP only from the point of view of someone who needs to reliably
transfer data though the network. But network architects and administrators often have
other goals. They want to make sure that <span class="emphasis"><i
class="EMPHASIS">all</i></span> users of a network can share the available bandwidth
equally and to assure that the network is always available. If TCP allowed all users to
send data whenever they wanted, bandwidth would not be shared equally and network
throughput might collapse in response to the load. Hence the focus of TCP is deciding
<span class="emphasis"><i class="EMPHASIS">when</i></span> to allow a user to send data.
The protocol architects who created TCP named it Transmission <span class="emphasis"><i
class="EMPHASIS">Control</i></span> Protocol to reflect this focus. Said differently, TCP
<span class="emphasis"><i class="EMPHASIS">adds latency</i></span> whenever necessary to
maintain equal bandwidth sharing and network stability. Latency-sensitive applications
typically don't want a transport protocol deciding when they can send.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="TCP-LATENCY-BEHAVIOR" name="TCP-LATENCY-BEHAVIOR">2.1. TCP
Latency Behavior</a></h3>

<p>Broadly speaking, the way that TCP does rate control makes it unsuitable for
latency-sensitive applications. (See <a href="#GROUP-RATE-CONTROL">Section 3</a> for a
contrast with other ways of doing rate control.) A TCP <span class="emphasis"><i
class="EMPHASIS">receiver</i></span> will add latency whenever packet loss or network
routing causes packets to arrive out of order. A TCP <span class="emphasis"><i
class="EMPHASIS">sender</i></span> will add latency when going faster would cause network
congestion or when it would be sending faster than the receiver can process the incoming
data.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="TCP-RECEIVER-SIDE-LATENCY"
name="TCP-RECEIVER-SIDE-LATENCY">2.2. TCP Receiver-Side Latency</a></h3>

<p>TCP only supports one delivery model: in-order delivery. This means that a TCP
receiver must add latency whenever data arrives out of order so as to put the data back
in order. TCP also often unnecessarily retransmits data that was already successfully
received after out-of-order data is received.</p>

<p>There are two main causes of out-of-order data reception. The most frequent cause is
packet loss, either at the physical or network-layer. Another, less frequent cause of
out-of-order data reception is that packets can take different paths through the network;
one path have more latency than another. In either case, TCP inserts latency to put the
packets back in order, as illustrated in <a href="#INORDER">Figure 1</a>.</p>

<div class="FIGURE"><a id="INORDER" name="INORDER"></a>
<p><b>Figure 1. In-Order Delivery</b></p>

<p><img src="InOrder.png" align="CENTER" /></p>
</div>

<p>Note that the receiving application cannot receive packet 3 until (the retransmitted)
packet 2 has arrived. The receiving TCP stack adds latency while waiting for the
successful arrival of packet 2 before packet 3 can be delivered.</p>

<p>Contrast this with the case where a transport with an arrival-order delivery model is
used as shown in <a href="#ARRIVALORDER">Figure 2</a>.</p>

<div class="FIGURE"><a id="ARRIVALORDER" name="ARRIVALORDER"></a>
<p><b>Figure 2. Arrival-Order Delivery</b></p>

<p><img src="ArrivalOrder.png" align="CENTER" /></p>
</div>

<p>Note that packet 3 is delivered to the application layer as soon as it arrives.</p>

<p>TCP cannot provide arrival order delivery, but UDP can. However, simple UDP is awkward
for many applications because it provides no reliability in delivery. In the above
example, it becomes the application's responsibility to detect packet 2's loss and
request its retransmission.</p>

<p>With this in mind, <a href="http://www.29West.Com/" target="_top">29West</a> created a
reliable multicast protocol called LBT-RM. It offers lower latency than TCP since it uses
UDP and arrival order delivery, but it can also provide reliability through the built-in
loss detection and retransmission logic. Of course, each message delivered by LBT-RM has
a sequence number so that the application has message sequencing information available if
needed.</p>

<p>If you're curious about how much latency is being added by the TCP in-order delivery
model, you can often get a hint by looking at output from the <tt class="COMMAND">netstat
-s</tt> command. Look for statistics from TCP. In particular, look for out-of-order
packets received and total packets received. Divide these two numbers to get a percentage
of packets that are being delayed by TCP in-order delivery.</p>

<p>For example, consider this output from <tt class="COMMAND">netstat -s</tt>.</p>

<pre class="SCREEN">
tcp:
. . .
      2854 packets received
. . .
          172 out-of-order packets (151915 bytes)
</pre>

<p>The above example statistics show that about 6% of all incoming TCP packets are being
delayed due the requirement of in-order delivery (this is pretty high; we hope that you
are not experiencing this degree of packet loss). Be aware that <tt
class="COMMAND">netstat</tt> output tends to vary quite a bit from operating system to
operating system, so your output may look quite different from the above.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="TCP-SENDER-SIDE-LATENCY" name="TCP-SENDER-SIDE-LATENCY">2.3.
TCP Sender-Side Latency</a></h3>

<p>TCP is designed to send as fast as possible while maintaining equal bandwidth sharing
with all other TCP streams and reliable delivery. A TCP sender may slow down for two
reasons:</p>

<ol type="1">
<li>
<p>Going faster would cause network congestion.</p>
</li>

<li>
<p>Going faster would send data faster than the receiver can process it.</p>
</li>
</ol>

<p>If you were to liken TCP to an automobile driver, you'd have to call TCP the most
lead-footed yet considerate driver on the road. As long as the road ahead is clear, TCP
will drive as fast as possible. But as soon as other traffic appears on the road, TCP
will slow down so that other drivers (i.e. other TCP streams) have a equal shot at using
the road too. The technical term for this behavior is <span class="emphasis"><i
class="EMPHASIS">congestion control</i></span>. When a network becomes congested, the TCP
algorithms insure that the pain is felt equally by all active TCP streams, which is
exactly the behavior you want if your primary concern is bandwidth sharing and network
stability. However, if you have a latency-sensitive application, you would prefer that it
get priority, leaving the other less-critical applications to divide up the remaining
bandwidth. That is, you would like to be able to add flashing lights and a siren to your
latency-sensitive applications.</p>

<p>It's important to note that TCP uses two different mechanisms to measure congestion:
round-trip time (RTT) and packet loss. (See <a href="#MYTH-ALL-PACKET-LOSS-BAD">Myth: All
Packet Loss is Bad</a> for details.) Sticking with the automobile analogy, an increase in
RTT causes TCP to take its foot off the gas to coast while packet loss causes it to step
on the brakes. TCP cuts its transmission rate by 1/2 every time it detects loss. This can
add additional latency after the original loss. Even small amounts of loss can
drastically cut TCP throughput.</p>

<p>TCP also makes sure that the sender does not send data faster than the receiver can
process it. A certain amount of buffer space is allocated on the receiver, and when that
buffer fills, the sender is blocked from sending more. The technical term for this
behavior is <span class="emphasis"><i class="EMPHASIS">flow control</i></span>. TCP's
philosophy is "better late than never". Many latency-sensitive applications prefer the
"better never than late" philosophy. At the very least, a slow, possibly problematic
receiver should not cause unnecessary latency with other receivers. Obviously, there
needs to be enough buffer space to handle normal periods of traffic bursts and temporary
receiver slow-downs, but if/when those buffers do fill up, it's better to drop data than
block the sender.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="TCP-LATENCY-RECOMMENDATIONS"
name="TCP-LATENCY-RECOMMENDATIONS">2.4. TCP Latency Recommendations</a></h3>

<p>It's probably best to avoid using TCP for latency-sensitive applications. As an
alternative, consider protocols that slow down a sender only when required for network
stability. For example, the <a href="http://www.29West.Com/" target="_top">29West</a> <b
class="APPLICATION">LBM</b> product offers protocols like LBT-RM that have been designed
specifically for latency-sensitive applications. Such protocols allow reliable yet
latency-bounded delivery without sacrificing network stability.</p>

<p>When TCP cannot be avoided, it's probably best to:</p>

<ul>
<li>
<p>Use clever buffering and non-blocking sockets with TCP to drop data for congested TCP
connections when data loss is preferable over latency. <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> uses such buffering for its latency-bounded TCP
feature.</p>
</li>

<li>
<p>Consider disabling <a href="http://en.wikipedia.org/wiki/Nagle%27s_algorithm"
target="_top">Nagle's algorithm</a> with the <code class="PARAMETER">TCP_NODELAY</code>
option to <code class="FUNCTION">setsockopt()</code>. Although this may produce decreased
latency, it often does so at the expense of efficiency.</p>
</li>

<li>
<p>Route latency-sensitive TCP traffic over a dedicated network where congestion is
minimized or eliminated.</p>
</li>

<li>
<p>Keep kernel socket buffers to a minimum so that latency is not added as data is stored
in such buffers. TCP windows substantially larger than the <a
href="http://en.wikipedia.org/wiki/Bandwidth_delay_product" target="_top">bandwidth-delay
product (BDP)</a> for the network will not increase TCP throughput but they can add
latency.</p>
</li>
</ul>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="GROUP-RATE-CONTROL" name="GROUP-RATE-CONTROL">3. Group Rate
Control</a></h2>

<p>Any application seeking to deliver the same data stream to a group of receivers faces
challenges in dealing with slow receivers. Group members that <span class="emphasis"><i
class="EMPHASIS">can</i></span> keep up with the sender may be inconvenienced or perhaps
even harmed by those that <span class="emphasis"><i class="EMPHASIS">can't</i></span>
keep up. At the very least, slow receivers cause the sender to use memory for buffering
that could perhaps be put to other uses. Buffering adds latency, at least for the slow
receiver and perhaps for all receivers. Note that rate control issues are present for
groups using either multicast or unicast addressing.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="CRYBABY-RECEIVER" name="CRYBABY-RECEIVER">3.1. The Crybaby
Receiver Problem</a></h3>

<p>Often, the whole group suffers due to the problems of one member. In extreme cases,
the throughput for the group falls to zero because resources that the group shares are
dedicated to the needs of one or a few members. Examples of shared resources include the
sender's CPU, memory, and network bandwidth. This phenomenon is sometimes called the
"crybaby receiver problem" because the cries (e.g. retransmission requests) from one
receiver dominate the attention of the parent (the sender).</p>

<p>The chance of encountering a crybaby receiver problem increases as the number of
receivers in the group increases. Odds are that at least one receiver will be having
difficulty keeping up if the group is large enough.</p>

<p>As long as all receivers in the group are best served by the sender running at the
same speed, there is no conflict within the group. Scenarios such as crybaby receivers
often present a conflict between what's best for a few members and what's best for the
majority. Robust systems will have policies for dealing with the apparent conflict and
consistently resolving it.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="GROUP-RATE-CONTROL-POLICIES"
name="GROUP-RATE-CONTROL-POLICIES">3.2. Group Rate Control Policies</a></h3>

<p>There are three rate control policies that a sender can use for dealing with such
conflict within a group. Two are extremes and the third is a middle ground.</p>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="POLICY-EXTREME1" name="POLICY-EXTREME1">3.2.1. Policy Extreme
1</a></h4>

<p>The sender slows down to the rate of the slowest receiver. If the sender cannot
control the rate at which new data arrives for transmission to the group, it must either
buffer the data until the slowest receiver is ready or drop it. <span class="emphasis"><i
class="EMPHASIS">All</i></span> receivers in the group then experience latency or lost
data.</p>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="POLICY-EXTREME2" name="POLICY-EXTREME2">3.2.2. Policy Extreme
2</a></h4>

<p>The sender sends as fast as is convenient for it. This is often the rate at which new
data arrives or is generated. It is often possible that this rate is too fast for even
the fastest receivers. All receivers that can't keep up with the rate most convenient for
the sender will experience lost data. This is often called "uncontrolled" since there is
no mechanism to regulate the rate used by the sender.</p>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="POLICY-MIDDLEGROUND" name="POLICY-MIDDLEGROUND">3.2.3. Middle
Ground</a></h4>

<p>The sender operates within a set of boundaries established a system administrator or
architect. The sender's goal is to minimize data loss and latency in the receiver group
while staying within the configured limits.</p>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="GROUP-RATE-CONTROL-CONSEQUENCES"
name="GROUP-RATE-CONTROL-CONSEQUENCES">3.3. Group Rate Control Consequences</a></h3>

<p>The extreme policies have potentially dire consequences for many applications. For
example, neither is ideal for market data and other types of latency-sensitive data.</p>

<p>Extreme 2 is the policy most often used. Successful use of it requires that networks
and receivers be provisioned to keep up with the fastest rate that might be convenient
for the sender. Such policies often leave little bandwidth for TCP traffic and can be
vulnerable to "NAK storms" and other maladies which can destabilize the entire
network.</p>

<p>Extreme 1 is appropriate only for transactional applications where it's more important
for the group to stay in sync than for the group to have low latency.</p>

<p>The middle ground policy is ideal for many latency-sensitive applications such as
transport of financial market data. It allows for low-latency reliable delivery while
maintaining the stability of the network. No amount of overload can cause a "NAK storm"
or other network outage when the policy is established with knowledge of the capabilities
of the network.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="POLICY-SELECTION" name="POLICY-SELECTION">3.4. Policy
Selection</a></h3>

<p>The need to establish a group rate control policy is often not apparent to those
accustomed to dealing with two-party communication (e.g. TCP). When there are only two
parties communicating, one policy is commonly used: the sender goes as fast as it can
without going faster than the receiver or being unfair to others on the network. (See <a
href="#TCP-LATENCY">Section 2</a> for details.) This is the only sensible policy for
applications that can withstand some latency and cannot withstand data loss. With
two-party communication using TCP, it's the only policy choice you have. However, group
communication with one sender and many receivers opens up all of the policy possibilities
mentioned above.</p>

<p>Some messaging systems support only one policy and hence require no policy
configuration. Others may allow a choice of policy. Any middle ground policy will need to
be configured to establish the boundaries within which it should operate.</p>

<p>The best results are obtained when the specific needs of a messaging application have
been considered and the messaging system has been configured to reflect them. A messaging
system has no means to automatically select from among the available group rate control
policies. Human judgment is required.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="GROUP-RATE-CONTROL-RECOMMENDATIONS"
name="GROUP-RATE-CONTROL-RECOMMENDATIONS">3.5. Group Rate Control
Recommendations</a></h3>

<p>A group rate control policy should be chosen to match the needs of the application
serving the group. The choice is often made by weighing the benefits of low latency
against reliable delivery. These benefits have to be considered for individuals within
the group and for the group as a whole.</p>

<p>In some applications, members of the group benefit from the presence of other members.
In these applications, the group benefit from reliable reception among all receivers may
outweigh the pain of added latency or limited group throughput. Consider a group of
servers that share the load from a common set of clients. Assume that the servers have to
stay synchronized with a stream of messages to be able to answer client queries. If one
server leaves the group because it lost some messages, the clients it was serving would
move to the remaining servers. This could lead to a domino effect where a traffic burst
caused the slowest server to drop from the group which in turn increased the load on the
other servers causing them to fail in turn as well. Clearly in a situation like this,
it's better for the whole group of servers to slow down a bit during a traffic peak so
that even the slowest among them can keep up without loss. The appropriate group rate
control policy for such an application is Extreme 1 (see <a
href="#POLICY-EXTREME1">Section 3.2.1</a>).</p>

<p>Other applications see no incremental benefit for the group if all members experience
reliable reception. Consider a group of independent traders who all subscribe to a market
data stream. Traders who can keep up with the rate convenient for the sender do not want
the sender to slow down for those who can't. Extreme 2 (see <a
href="#POLICY-EXTREME2">Section 3.2.2</a>) is probably the appropriate policy for an
application like this. However, care must be taken to prevent the sender from going
faster than even the fastest trader as that often leads to NAK storms from which there is
no recovery.</p>

<p><a href="http://www.29West.Com/" target="_top">29West</a> recommends a careful
analysis of the policies that are best for the group using an application and for
individual members of the group. The rate control policy best suited to your application
will generally emerge from such an analysis. We have found that it is possible to build
stable, low-latency messaging systems with careful network design and a messaging layer
like <a href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> that supports a middle ground policy through the use of
rate controls.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="GROUP-RATE-CONTROL-PROTOCOLS"
name="GROUP-RATE-CONTROL-PROTOCOLS">3.6. Group Rate Control and Transport
Protocols</a></h3>

<p>Once you've chosen a group rate control policy appropriate for your application, it's
important to chose a transport protocol that can implement your chosen policy. Some
transport protocols offer only the extreme policies while others allow parameters to be
set to implement a middle ground policy.</p>

<p>UDP provides no rate control at all, so it follows group policy Extreme 2 above (see
<a href="#POLICY-EXTREME2">Section 3.2.2</a>).</p>

<p>TCP doesn't operate naturally as a group communication protocol since it only supports
unicast addressing. However, when TCP is used to send copies of the same data stream to
more than one receiver, all of the group rate control issues discussed above are present.
TCP's inherent flow control feature follows group policy Extreme 1 described above (see
<a href="#POLICY-EXTREME1">Section 3.2.1</a>). If the sender is willing to use
non-blocking I/O and manage buffering, then Extreme 2 and middle ground policies can be
implemented to some degree. For example <a href="http://www.29West.Com/products/lbm/"
target="_top"><b class="APPLICATION">LBM</b></a> supports middle ground policies over TCP
with its latency-bounded TCP feature.</p>

<p>Some reliable multicast transport protocols provide no rate control at all (e.g. TIBCO
Rendezvous), some provide only a fixed maximum rate limit (e.g. PGM), and some provide
separate rate controls for initial data transmission and retransmission (e.g. LBT-RM from
<a href="http://www.29West.Com/" target="_top">29West</a>). See <a
href="#MULTICAST-RETRANSMISSIONS">Section 15</a> for details.</p>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="ETHERNET-FLOW-CONTROL" name="ETHERNET-FLOW-CONTROL">4.
Ethernet Flow Control</a></h2>

<p>In the process of testing the LBT-RM reliable multicast protocol used in the <a
href="http://www.29West.Com/" target="_top">29West</a> <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> messaging product, we noticed that some Ethernet switches
and NICs now implement a form of flow control that can cause unexpected results. This is
commonly seen in mixed speed networks (e.g. 10/100 Mbps or 100/1000 Mbps). We have seen
apparently strange results such as two machines being able to send TCP at 100 Mbps but
being limited to 10 Mbps for multicast using LBT-RM.</p>

<p>The cause of this seems to be Ethernet switches and NICs that implement the IEEE
802.3x Ethernet flow control standard. It seems to be most commonly implemented in
desktop-grade switches. <a href="http://www.nwfusion.com/netresources/0913flow2.html"
target="_top">Infrastructure-grade equipment makers seem to have identified the potential
harm that Ethernet flow control can cause and decided to implement it carefully if at
all</a>.</p>

<p>The intent of Ethernet flow control is to prevent loss in switches by providing back
pressure to the sending NIC on ports that are going too fast to avoid loss. While this
sounds like a good idea on the surface, higher-layer protocols like TCP were designed to
rely on loss as a signal that they should send more slowly. Hence, at the very least,
Ethernet flow control duplicates the flow control mechanism already built into TCP. At
worst, it unnecessarily slows senders when there's no need.</p>

<p>Problems can happen with multicast on mixed-speed networks if a switch with Ethernet
flow control applies back pressure on a multicast sender when it tries to go faster than
the slowest active port on the switch. Consider the scenario where two machines X and Y
are connected to a switch at 100 Mbps while a third machine Z is connected to the same
switch at 10 Mbps. X can send TCP traffic to Y at 100 Mbps since the switch sees no
congestion delivering data to Y's switch port. But if X starts sending multicast, the
switch tries to forward the multicast to Z's switch port as well as to Y's. Since Z is
limited to 10 Mbps, the switch senses congestion and prevents X from sending faster than
10 Mbps.</p>

<p>We have tested switches that behave this way even if there are no multicast receivers
on the slow ports. That is, these switches did not have an IGMP snooping feature or an
equivalent that would prevent multicast traffic from being forwarded out ports where
there were no interested receivers. It seems best to avoid such switches when using
LBT-RM. Any production system with switches like this may be vulnerable to the problem
where the network runs fine until one day somebody plugs an old laptop into a conference
room jack and inadvertently limits all the multicast sources to 10 Mbps.</p>

<p>It may also be possible to configure hosts to ignore flow control signals from
switches. For example, on Linux, the <tt class="COMMAND">/sbin/ethtool</tt> command may
be able to configure a host to ignore pause signals that are sent from a switch sensing
congestion. Module configuration parameters or registry settings might also be
appropriate.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="PACKET-LOSS-MYTHS" name="PACKET-LOSS-MYTHS">5. Packet Loss
Myths</a></h2>

<p>Packet loss is a common occurrence that is often misunderstood. Many applications can
run successfully with little or no attention paid to packet loss. However,
latency-sensitive applications can often benefit from improved performance if packet loss
is better understood. In our work at <a href="http://www.29West.Com/"
target="_top">29West</a> deploying our <a href="http://www.29West.Com/products/lbm/"
target="_top"><b class="APPLICATION">LBM</b></a> product, we have encountered many myths
surrounding packet loss. Following paragraphs address these myths and provide links to
additional helpful information.</p>

<div class="QANDASET">
<dl>
<dt><a href="#MYTH-TRANSMISSION-ERRORS"><span class="emphasis"><i
class="EMPHASIS">Myth</i></span>--Most loss is caused by transmission errors, gamma rays,
etc.</a></dt>

<dt><a href="#MYTH-EXPECTING-ZERO-LOSS"><span class="emphasis"><i
class="EMPHASIS">Myth</i></span>--There is no loss in networks that are operating
properly.</a></dt>

<dt><a href="#MYTH-LOSS-ONLY-IN-NETWORKS"><span class="emphasis"><i
class="EMPHASIS">Myth</i></span>--Loss only happens in networks (i.e. not in
hosts).</a></dt>

<dt><a href="#MYTH-ALL-PACKET-LOSS-BAD"><span class="emphasis"><i
class="EMPHASIS">Myth</i></span>--All packet loss is bad.</a></dt>

<dt><a href="#MYTH-UNICAST-AND-MULTICAST-LOSS-GO-TOGETHER"><span class="emphasis"><i
class="EMPHASIS">Myth</i></span>--Unicast and multicast loss always go together.</a></dt>
</dl>

<div class="QANDAENTRY">
<div class="QUESTION">
<p><a id="MYTH-TRANSMISSION-ERRORS" name="MYTH-TRANSMISSION-ERRORS"></a> <b></b> <span
class="emphasis"><i class="EMPHASIS">Myth</i></span>--Most loss is caused by transmission
errors, gamma rays, etc.</p>
</div>

<div class="ANSWER">
<p><b></b> <span class="emphasis"><i class="EMPHASIS">Reality</i></span>--Our experience
and anecdotal evidence from others indicates that buffer overflow is the most common
cause of packet loss. These buffers may be in network hardware (e.g. switches and
routers) or it may be in operating systems. See <a
href="#UDP-BUFFERING-BACKGROUND">Section 7</a> for background information.</p>
</div>
</div>

<div class="QANDAENTRY">
<div class="QUESTION">
<p><a id="MYTH-EXPECTING-ZERO-LOSS" name="MYTH-EXPECTING-ZERO-LOSS"></a> <b></b> <span
class="emphasis"><i class="EMPHASIS">Myth</i></span>--There is no loss in networks that
are operating properly.</p>
</div>

<div class="ANSWER">
<p><b></b> <span class="emphasis"><i class="EMPHASIS">Reality</i></span>--The normal
operation of TCP congestion control may cause loss due to queue overflow. See <a
href="http://public.lanl.gov/radiant/pubs/traffic/icdcs00-tcp.pdf" target="_top">this
report</a> for more information. Loss rates of several percent were common under heavy
congestion.</p>
</div>
</div>

<div class="QANDAENTRY">
<div class="QUESTION">
<p><a id="MYTH-LOSS-ONLY-IN-NETWORKS" name="MYTH-LOSS-ONLY-IN-NETWORKS"></a> <b></b>
<span class="emphasis"><i class="EMPHASIS">Myth</i></span>--Loss only happens in networks
(i.e. not in hosts).</p>
</div>

<div class="ANSWER">
<p><b></b> <span class="emphasis"><i class="EMPHASIS">Reality</i></span>--The flow
control mechanism of TCP should prevent packet loss due to host buffer overflows.
However, UDP contains no flow-control mechanism leaving the possibility that UDP receiver
buffers will overflow. Hosts receiving high-volume UDP traffic often experience internal
packet loss due to UDP buffer overflow. See <a href="#UDP-BUFFER-VS-TCP-BUFFER">Section
7.6</a> for more on the contrast between TCP buffering and UDP buffering. See <a
href="#DETECTING-UDP-LOSS">Section 8.9</a> for advice on detecting UDP buffer overflow in
a host.</p>
</div>
</div>

<div class="QANDAENTRY">
<div class="QUESTION">
<p><a id="MYTH-ALL-PACKET-LOSS-BAD" name="MYTH-ALL-PACKET-LOSS-BAD"></a> <b></b> <span
class="emphasis"><i class="EMPHASIS">Myth</i></span>--All packet loss is bad.</p>
</div>

<div class="ANSWER">
<p><b></b> <span class="emphasis"><i class="EMPHASIS">Reality</i></span>--Packet loss
plays at least two important, beneficial roles:</p>

<ul>
<li>
<p>Implicit signaling of congestion: TCP uses loss to discover contention with other TCP
streams. Each TCP stream passing through a congestion point must dynamically discover the
existence of other streams sharing the congestion point in order to fairly share the
available bandwidth. They do this by monitoring 1) the SRTT (Smoothed Round-Trip Time) as
an indication of queue depth, and 2) packet loss as an indication of queue overflow.
Hence TCP relies upon packet loss as an implicit signal of network congestion. See <a
href="#TCP-SENDER-SIDE-LATENCY">Section 2.3</a> for discussion of the impact this can
have on latency.</p>
</li>

<li>
<p>Efficient discarding of stale, latency-sensitive data: See <a
href="#UDP-BUFFER-OPTIMAL-SIZE">Section 8.1</a> for more information.</p>
</li>
</ul>
</div>
</div>

<div class="QANDAENTRY">
<div class="QUESTION">
<p><a id="MYTH-UNICAST-AND-MULTICAST-LOSS-GO-TOGETHER"
name="MYTH-UNICAST-AND-MULTICAST-LOSS-GO-TOGETHER"></a> <b></b> <span class="emphasis"><i
class="EMPHASIS">Myth</i></span>--Unicast and multicast loss always go together.</p>
</div>

<div class="ANSWER">
<p><b></b> <span class="emphasis"><i class="EMPHASIS">Reality</i></span>--Unicast and
multicast traffic often follows different forwarding paths in networks and hosts. Just
because unicast (TCP) reports no loss, you can't assume that there is no multicast
loss.</p>
</div>
</div>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MONITORING-MESSAGING-SYSTEMS"
name="MONITORING-MESSAGING-SYSTEMS">6. Monitoring Messaging Systems</a></h2>

<p>Production deployments of messaging systems often employ real-time monitoring and
rapid human intervention if something goes wrong. Urgent reaction to problems detected by
real-time monitoring can be required to prevent the messaging system from becoming
unstable. A common example of this is using real-time monitoring to discover crybaby
receivers and repair or remove them from the network. (See <a
href="#CRYBABY-RECEIVER">Section 3.1</a> for details.) The presence of a crybaby receiver
can starve lossless receivers of bandwidth needed for new messages in what is commonly
called a NAK storm.</p>

<p><a href="http://www.29West.Com/" target="_top">29West</a> has designed <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> so that real-time monitoring and urgent response are not
required. The design of LBM encourages stable operation by allowing you to pre-configure
how LBM will use resources under all traffic and network conditions. Hence manual
intervention is not required when those conditions occur.</p>

<p>Monitoring LBM still fills important roles other than maintaining stable operation.
Chiefly among these are capacity planning and gaining a better understanding the latency
that LBM adds to recover from loss. Collecting accumulated statistics from all sources
and all receivers once per day is generally adequate for these purposes.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="UDP-BUFFERING-BACKGROUND" name="UDP-BUFFERING-BACKGROUND">7.
UDP Buffering Background</a></h2>

<p>In our work at <a href="http://www.29West.Com/" target="_top">29West</a> with <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a>, we have found that successful deployment of our reliable
multicast (LBT-RM) and reliable unicast (LBT-RU) protocols often depends on familiarity
with UDP buffering in operating systems. Our reliable unicast and reliable multicast
protocols use UDP to achieve control of transport latency that would be impossible with
TCP.</p>

<p>Much of what we've learned deploying our UDP-based protocols should be applicable to
other high-performance work with UDP. We have collected here some of the background
information that we've found helpful in understanding issues related to UDP
buffering.</p>

<p>Although UDP is buffered on both the send and receive side, we've seldom seen a need
to be concerned with send-side UDP buffering. For brevity, we'll use simply "UDP
buffering" to refer to receive-side UDP buffering.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-FUNCTION" name="UDP-BUFFER-FUNCTION">7.1. Function
of UDP Buffering</a></h3>

<p>UDP packets may arrive in bursts because they were sent rapidly or because they were
bunched together by the normal buffering action of network switches and routers.</p>

<p>Similarly, UDP packets may be consumed rapidly when CPU time is available to run the
consuming application. Or they may be consumed slowly because CPU time is being used to
run other processes.</p>

<p>UDP receive buffering serves to match the arrival rate of UDP packets (or "datagrams")
with their consumption rate by an application program. Of course, buffering cannot help
cases where the long-term average send rate exceeds the average receive rate.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-IMPORTANCE" name="UDP-BUFFER-IMPORTANCE">7.2.
Importance of UDP Buffering</a></h3>

<p>UDP receive buffering is done in the operating system kernel. Typically the kernel
allocates a fixed-size buffer for each socket receiving UDP. Buffer space is consumed for
every UDP packet that has arrived but has not yet been delivered to the consuming
application. Unused space is generally unavailable for other purposes because it must be
readily available for the possible arrival of more packets. See <a
href="#UDP-BUFFER-LIMIT-RATIONALE">Section 7.4</a> for a further explanation.</p>

<p>If a UDP packet arrives for a socket with a full buffer, it is discarded by the kernel
and a counter is incremented. See <a href="#DETECTING-UDP-LOSS">Section 8.9</a> for
information on detecting UDP loss. A common myth is that all UDP loss is bad. (See <a
href="#MYTH-ALL-PACKET-LOSS-BAD">Myth: All Packet Loss is Bad</a>.) Even if it's not all
bad, UDP loss does have its consequences. (See <a href="#UDP-LOSS-CONSEQUENCES">Section
8.3</a>.)</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-ROLES" name="UDP-BUFFER-ROLES">7.3. Kernel and User
Mode Roles in UDP Buffering</a></h3>

<p>The memory required for the kernel to do UDP buffering is a scarce resource that the
kernel tries to allocate wisely. (See <a href="#UDP-BUFFER-LIMIT-RATIONALE">Section
7.4</a> for more on the rationale.) Applications expecting low-volume UDP traffic or
those expecting low CPU scheduling latency (see <a
href="#LATENCY-SOURCES-CPU-SCHEDULING">Section 17.6</a>) need not consume very much of
this scarce resource. Applications expecting high-volume UDP traffic or those expecting a
high CPU scheduling latency may be justified in consuming more UDP buffer space than
others.</p>

<p>Typically, the kernel allocates a modest-size buffer when a UDP socket is created.
This is generally adequate for less-demanding applications. Applications requiring a
larger UDP receive buffer can request it with the system call <code
class="FUNCTION">setsockopt(...SO_RCVBUF...)</code>.</p>

<p>Explicit configuration of the application may be required before it will request a
larger UDP buffer. (For <b class="APPLICATION">LBM</b>, this is the context option <code
class="PARAMETER">transport_lbtrm_receiver_socket_buffer</code>.)</p>

<p>The kernel configuration will allow such requests to succeed only up to a set size
limit. This limit may often be increased by changing the kernel configuration. See <a
href="#SETTING-KERNEL-UDP-BUFFER-LIMITS">Section 8.8</a> for information on setting
kernel UDP buffer limits.</p>

<p>Hence two steps are often required to get adequate UDP buffer space:</p>

<ul>
<li>
<p>Change the kernel configuration to increase the limit on the largest UDP buffer
allocation that it will allow.</p>
</li>

<li>
<p>Change the application to request a larger UDP buffer.</p>
</li>
</ul>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-LIMIT-RATIONALE"
name="UDP-BUFFER-LIMIT-RATIONALE">7.4. Unix Kernel UDP Buffer Limit Rationale</a></h3>

<p>It may seem to some that the default maximum UDP buffer size on many Unix kernels is a
bit stingy. Understanding the rationale behind these limits may help.</p>

<p>UDP is typically used for low-volume query/response work (e.g. DNS, NTP, etc.). The
kernel default limits assume that UDP will be used in this way.</p>

<p>UDP kernel buffer space is allocated from physical memory for the exclusive use of one
process. The kernel tries to make sure that one process can't starve others for physical
memory by allocating large UDP buffers that exhaust all the physical memory on a
machine.</p>

<p>To some degree, the meager default limits are a legacy from the days when 4 MB was a
lot of physical memory. In these days when several gigabytes of physical memory space is
common, such small default limits seem particularly stingy.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-CPU-SCHED" name="UDP-BUFFER-CPU-SCHED">7.5. UDP
Buffering and CPU Scheduling Latency</a></h3>

<p>For the lowest-possible latency, the operating system would run a process wishing to
receive a UDP packet as soon as the packet arrives. In practice, the operating system may
allow other processes to finish using their CPU time slice first. It may also seek to
improve efficiency by accumulating several UDP packets before running the application. We
will call the time that elapses between when a UDP packet arrives and when the consuming
application gets to run on a CPU the CPU scheduling latency. See <a
href="#LATENCY-SOURCES-CPU-SCHEDULING">Section 17.6</a> for more information. UDP buffer
space fills during CPU scheduling latency and empties when the consuming process runs on
a CPU. CPU scheduling latency plays a key role optimal UDP buffer sizing. See <a
href="#UDP-BUFFER-OPTIMAL-SIZE">Section 8.1</a> for more information.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-VS-TCP-BUFFER" name="UDP-BUFFER-VS-TCP-BUFFER">7.6.
TCP Receive Buffering vs. UDP Receive Buffering</a></h3>

<p>The operating system kernel automatically allocates TCP receive buffer space based on
policy settings, available memory, and other factors. TCP in the sending kernel
continuously monitors available receive buffer space in the receiving kernel. When a TCP
receive buffer fills up, the sending kernel prevents the sending application from using
CPU time to generate any more data for the connection. This behavior is called "flow
control." In a nutshell, it prevents the receiver buffer space from overflowing by adding
latency at the sender. We say that the speed of a TCP sender is "receiver-paced" because
the sending application is prevented from sending when data cannot be delivered to the
receiving application.</p>

<p>The OS kernel also allocates UDP receive buffer space based on policy settings.
However, UDP senders do not monitor available UDP receive buffer space in the receiving
kernel. UDP receivers simply discard incoming packets once all available buffer space is
exhausted. We say that the speed of a UDP sender is "sender-paced" because the sending
application can send whenever it wants without regard to available buffer space in the
receiving kernel.</p>

<p>The default TCP buffer settings are generally adequate and usually require adjustment
only for unusual network parameters or performance goals.</p>

<p>The appropriate size for an application's UDP receive buffer is influenced by factors
that cannot be known when operating system default policies are established. Further,
there is nothing the operating system can do to automatically discover the appropriate
size. An application may know that it would benefit from a UDP receive buffer larger or
smaller than the default used by the operating system. It can request a non-default UDP
buffer size from the operating system, but the request may not be granted due to a policy
limit. See <a href="#UDP-BUFFER-LIMIT-RATIONALE">Section 7.4</a> for reasons behind such
policy limits. Operating system policies may have to be adjusted to successfully run
high-performance UDP applications like <b class="APPLICATION">LBM</b>. See <a
href="#SETTING-KERNEL-UDP-BUFFER-LIMITS">Section 8.8</a> to change operating system
policies.</p>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="UDP-BUFFER-SIZING" name="UDP-BUFFER-SIZING">8. UDP Buffer
Sizing</a></h2>

<p>There are many questions surrounding UDP buffer sizing. What is the optimal size? What
are the consequences of an improperly sized UDP buffer? What are the equations needed to
compute an appropriate size for a UDP buffer? What default limit will the OS kernel place
on UDP buffer size and how can I change it? How can I tell if I'm having UDP loss
problems due to buffers that are too small? Answers to these questions and more are given
in the following sections.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-OPTIMAL-SIZE" name="UDP-BUFFER-OPTIMAL-SIZE">8.1.
Optimal UDP Buffer Sizing</a></h3>

<p>UDP buffer sizes should be large enough to allow an application to endure the normal
variance in CPU scheduling latency without suffering packet loss. They should also be
small enough to prevent the application from having to read through excessively old data
following an unusual spike in CPU scheduling latency.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-SMALL-CONSEQUENCES"
name="UDP-BUFFER-SMALL-CONSEQUENCES">8.2. UDP Buffer Space Too Small:
Consequences</a></h3>

<p>Too little UDP buffer space causes the operating system kernel to discard UDP packets.
The resulting packet loss has consequences described below.</p>

<p>The kernel often keeps counts of UDP packets received and lost. See <a
href="#DETECTING-UDP-LOSS">Section 8.9</a> for information on detecting UDP loss due to
UDP buffer space overflow. A common myth is that all UDP loss is bad (see <a
href="#MYTH-ALL-PACKET-LOSS-BAD">Myth: All Packet Loss is Bad</a>).</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-LOSS-CONSEQUENCES" name="UDP-LOSS-CONSEQUENCES">8.3. UDP
Loss: Consequences</a></h3>

<p>In most cases, it's the secondary effects of UDP loss that matter most. That is, it's
the <span class="emphasis"><i class="EMPHASIS">reaction</i></span> to the loss that has
material consequences more so than the loss itself. Note that the consequences discussed
here are independent of the <span class="emphasis"><i class="EMPHASIS">cause</i></span>
of the loss. Inadequate UDP receive buffering is just one of the more common causes we've
encountered deploying <b class="APPLICATION">LBM</b>.</p>

<p>Consider these areas when assessing the consequences of UDP loss:</p>

<ul>
<li>
<p><span class="emphasis"><i class="EMPHASIS">Latency</i></span>--The time that passes
between the initial transmission of a UDP packet and the eventual successful reception of
a retransmission is latency that could have been avoided were it not for the intervening
loss.</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">Bandwidth</i></span>--UDP loss usually
results in requests for retransmission, unless more up-to-date information is expected
soon (e.g. in the case of stock quote updates). Bandwidth used for retransmissions may
become significant, especially in cases where there is a large amount of loss or a large
number of receivers experiencing loss. See <a href="#MULTICAST-RETRANSMISSIONS">Section
15</a> for more information on multicast retransmissions.</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">CPU Time</i></span>--UDP loss causes the
receiver to use CPU time to detect the loss, request one or more retransmissions, and
perform the repair. Note that efficiently dealing with loss among a group of receivers
requires the use of many timers, often of short-duration. Scheduling and processing such
timers generally requires CPU time in both the operating system kernel ("system time")
and in the application receiving UDP ("user time"). Additional CPU time is required to
switch between kernel and user modes.</p>

<p>On the sender, CPU time is used to process retransmission requests and to send
retransmissions as appropriate. As on the receiver, many timers are required for
efficient retransmission processing, thus requiring many switches between kernel and user
modes.</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">Memory</i></span>--UDP receivers that can
only process data in the order that it was initially sent must allocate memory while
waiting for retransmissions to arrive. UDP loss causes such receivers to receive data in
an order different than that used by the sender. Memory is used to restore the order in
which it was initially sent.</p>

<p>Even UDP receivers that <span class="emphasis"><i class="EMPHASIS">can</i></span>
process UDP packets in the order they arrive may not be able to tolerate duplication of
packets. Such receivers must allocate memory to track which packets have been
successfully processed and which have not.</p>

<p>UDP senders interested in reliable reception by their receivers must allocate memory
to retain UDP packets after their initial transmission. Retained packets are used to fill
retransmission requests.</p>
</li>
</ul>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-LARGE-CONSEQUENCES"
name="UDP-BUFFER-LARGE-CONSEQUENCES">8.4. UDP Buffer Space Too Large:
Consequences</a></h3>

<p>Even though too little UDP buffer space is definitely bad and more is generally
better, it is still possible to have too much of a good thing. Perhaps the two most
significant consequences of too much UDP buffer space are slower recovery from loss and
physical memory usage. Each of these is discussed in turn below.</p>

<ul>
<li>
<p><span class="emphasis"><i class="EMPHASIS">Slower Recovery</i></span>--To best
understand the consequences of too much UDP buffer space, consider a stream of packets
that regularly updates the current value of a rapidly-changing variable in every tenth
packet. Why buffer more than ten packets? Doing so would only increase the number of
stale packets that must be discarded at the application layer. Given a data stream like
this, it's generally better to configure a ten-packet buffer in the kernel so that no
more than ten stale packets have to be read by the application before a return to fresh
ones from the stream.</p>

<p>It's often counter-intuitive, but excessive UDP buffering can actually increase the
recovery time following a large packet loss event. UDP receive buffers should be sized to
match the latency budget allocated for CPU scheduling latency with knowledge of expected
data rates. See <a href="#MESSAGING-LATENCY-BUDGET">Section 16</a> for more information
on latency budgets. See <a href="#UDP-BUFFER-EQUATION-SIZE">Section 8.6</a> for a UDP
buffer sizing equation.</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">Physical Memory Usage</i></span>--It is
possible to exhaust available physical memory with UDP buffer space. Requesting a UDP
receive buffer of 32 MB and then invoking ten receiver applications uses 320 MB of
physical memory. See <a href="#UDP-BUFFER-LIMIT-RATIONALE">Section 7.4</a> for more
information.</p>
</li>
</ul>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-EQUATION-LATENCY"
name="UDP-BUFFER-EQUATION-LATENCY">8.5. UDP Buffer Size Equations: Latency</a></h3>

<p>Assuming that an average rate is known for a UDP data stream, the amount of latency
that would be added by a full UDP receive buffer can be computed as:</p>

<p>Max Latency = Buffer Size / Average Rate</p>

<div class="NOTE">
<blockquote class="NOTE">
<p><b>Note:</b> Take care to watch for different units in buffer size and average rate
(e.g. kilo<span class="emphasis"><i class="EMPHASIS">bytes</i></span> vs. mega<span
class="emphasis"><i class="EMPHASIS">bits</i></span> per second).</p>
</blockquote>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-EQUATION-SIZE" name="UDP-BUFFER-EQUATION-SIZE">8.6.
UDP Buffer Size Equations: Buffer Size</a></h3>

<p>Assuming that an average rate is known for a UDP data stream, the buffer size needed
to avoid loss a given worst case CPU scheduling latency can be computed as:</p>

<p>Buffer Size = Max Latency * Average Rate</p>

<div class="NOTE">
<blockquote class="NOTE">
<p><b>Note:</b> Since data rates are often measured in <span class="emphasis"><i
class="EMPHASIS">bits</i></span> per second while buffers are often allocated in <span
class="emphasis"><i class="EMPHASIS">bytes</i></span>, careful conversion may be
necessary.</p>
</blockquote>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="UDP-BUFFER-KERNEL-DEFAULTS"
name="UDP-BUFFER-KERNEL-DEFAULTS">8.7. UDP Buffer Kernel Defaults</a></h3>

<p>The kernel variable that limits the maximum size allowed for a UDP receive buffer has
different names and default values by kernel given in the following table:</p>

<div class="INFORMALTABLE"><a id="AEN376" name="AEN376"></a>
<table border="1" class="CALSTABLE">
<col />
<col />
<col />
<thead>
<tr>
<th>Kernel</th>
<th>Variable</th>
<th>Default Value</th>
</tr>
</thead>

<tbody>
<tr>
<td>Linux</td>
<td><tt class="LITERAL">net.core.rmem_max</tt></td>
<td>131071</td>
</tr>

<tr>
<td>Solaris</td>
<td><tt class="LITERAL">udp_max_buf</tt></td>
<td>262144</td>
</tr>

<tr>
<td>FreeBSD, Darwin</td>
<td><tt class="LITERAL">kern.ipc.maxsockbuf</tt></td>
<td>262144</td>
</tr>

<tr>
<td>AIX</td>
<td><tt class="LITERAL">sb_max</tt></td>
<td>1048576</td>
</tr>

<tr>
<td>Windows</td>
<td>None we know of</td>
<td>Seems to grant all reasonable requests</td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="SETTING-KERNEL-UDP-BUFFER-LIMITS"
name="SETTING-KERNEL-UDP-BUFFER-LIMITS">8.8. Setting Kernel UDP Buffer Limits</a></h3>

<p>The examples in this table give the commands needed to set the kernel UDP buffer limit
to 8 MB. Root privilege is required to execute these commands.</p>

<div class="INFORMALTABLE"><a id="AEN411" name="AEN411"></a>
<table border="1" class="CALSTABLE">
<col />
<col />
<thead>
<tr>
<th>Kernel</th>
<th>Command</th>
</tr>
</thead>

<tbody>
<tr>
<td>Linux</td>
<td><tt class="COMMAND">sysctl -w net.core.rmem_max=8388608</tt></td>
</tr>

<tr>
<td>Solaris</td>
<td><tt class="COMMAND">ndd -set /dev/udp udp_max_buf 8388608</tt></td>
</tr>

<tr>
<td>FreeBSD, Darwin</td>
<td><tt class="COMMAND">sysctl -w kern.ipc.maxsockbuf=8388608</tt></td>
</tr>

<tr>
<td>AIX</td>
<td><tt class="COMMAND">no -o sb_max=8388608</tt> (note: AIX only permits sizes of
1048576, 4194304 or 8388608)</td>
</tr>
</tbody>
</table>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="MAKING-CHANGES-SURVIVE-REBOOT"
name="MAKING-CHANGES-SURVIVE-REBOOT">8.8.1. Making Changes Survive Reboot</a></h4>

<p>The AIX command given above will change the current value <span class="emphasis"><i
class="EMPHASIS">and</i></span> automatically modify <tt
class="FILENAME">/etc/tunables/nextboot</tt> so that the change will survive rebooting.
Other platforms require additional work described below to make changes survive a
reboot.</p>

<p>For Linux and FreeBSD, simply add the sysctl variable setting given above to <tt
class="FILENAME">/etc/sysctl.conf</tt> leaving off the <tt class="COMMAND">sysctl -w</tt>
part.</p>

<p></p>

<p>We haven't found a convention for Solaris, but would love to hear about it if we've
missed something. We've had success just adding the <tt class="COMMAND">ndd</tt> command
given above to the end of <tt class="FILENAME">/etc/rc2.d/S20sysetup</tt>.</p>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="DETECTING-UDP-LOSS" name="DETECTING-UDP-LOSS">8.9. Detecting
UDP Loss</a></h3>

<p>Interpreting the output of <tt class="COMMAND">netstat</tt> is important in detecting
UDP loss. Unfortunately, the output varies considerably from one flavor of Unix to
another. Hence we can't give one set of instructions that will work with all flavors.</p>

<p>For each Unix flavor, we tested under normal conditions and then under conditions
forcing UDP loss while keeping a close eye on the output of <tt class="COMMAND">netstat
-s</tt> before and after the tests. This revealed the statistics that appeared to have a
relationship with UDP packet loss. Output from Solaris and FreeBSD <tt
class="COMMAND">netstat</tt> was the most intuitive; Linux and AIX much less so.
Following sections give the command we used and highlight the important output for
detecting UDP loss.</p>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="DETECTING-SOLARIS-UDP-LOSS"
name="DETECTING-SOLARIS-UDP-LOSS">8.9.1. Detecting Solaris UDP Loss</a></h4>

<p>Use <tt class="COMMAND">netstat -s</tt>. Look for <tt
class="LITERAL">udpInOverflows</tt>. It will be in the <tt class="LITERAL">IPv4</tt>
section, not in the <tt class="LITERAL">UDP</tt> section as you might expect. For
example:</p>

<pre class="SCREEN">
IPv4:
      udpInOverflows      = 82427
</pre>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="DETECTING-LINUX-UDP-LOSS"
name="DETECTING-LINUX-UDP-LOSS">8.9.2. Detecting Linux UDP Loss</a></h4>

<p>Use <tt class="COMMAND">netstat -su</tt>. Look for <tt class="LITERAL">packet receive
errors</tt> in the <tt class="LITERAL">Udp</tt> section. For example:</p>

<pre class="SCREEN">
Udp:
      38799 packet receive errors
</pre>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="DETECTING-WINDOWS-UDP-LOSS"
name="DETECTING-WINDOWS-UDP-LOSS">8.9.3. Detecting Windows UDP Loss</a></h4>

<p>The command, <tt class="COMMAND">netstat -s</tt>, doesn't work the same in <span
class="TRADEMARK">Microsoft</span>&reg; <span class="TRADEMARK">Windows</span>&reg; as it
does in other operating systems. Therefore, unfortunately, there is no way to detect UDP
buffer overflow in <span class="TRADEMARK">Microsoft</span> <span
class="TRADEMARK">Windows</span>.</p>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="DETECTING-AIX-UDP-LOSS" name="DETECTING-AIX-UDP-LOSS">8.9.4.
Detecting AIX UDP Loss</a></h4>

<p>Use <tt class="COMMAND">netstat -s</tt>. Look for <tt class="LITERAL">fragments
dropped (dup or out of space)</tt> in the <tt class="LITERAL">ip</tt> section. For
example:</p>

<pre class="SCREEN">
ip:
      77070 fragments dropped (dup or out of space)
</pre>
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="DETECTING-FREEBSD-UDP-LOSS"
name="DETECTING-FREEBSD-UDP-LOSS">8.9.5. Detecting FreeBSD and Darwin UDP Loss</a></h4>

<p>Use <tt class="COMMAND">netstat -s</tt>. Look for <tt class="LITERAL">dropped due to
full socket buffers</tt> in the <tt class="LITERAL">udp</tt> section. For example:</p>

<pre class="SCREEN">
udp:
      6343 dropped due to full socket buffers
</pre>
</div>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MULTICAST-LOOPBACK" name="MULTICAST-LOOPBACK">9. Multicast
Loopback</a></h2>

<p>It's widely known that two processes running on same machine can use the loopback
network pseudo-interface with address 127.0.0.1 for communication even when there's no
real network interface present. The loopback interface neatly solves the problem of
providing a usable destination address when there are no other interfaces or when those
interfaces are unusable because they are not active. An example of an inactive interface
might be an Ethernet interface with no link integrity (or carrier detect) signal. Hence
an unplugged laptop may be used to develop unicast applications simply by configuring
those applications to use the loopback interface's address of 127.0.0.1.</p>

<p>Unfortunately, the kernel's usual loopback interface isn't used for multicast and
isn't capable of forwarding multicast from application to application on the same
machine. Ethernet interfaces <span class="emphasis"><i class="EMPHASIS">are</i></span>
capable of forwarding multicast, but <span class="emphasis"><i
class="EMPHASIS">only</i></span> when they are connected to a network. This is seldom an
issue on desktops that are generally connected, but it does come up with laptops that
tend to spend some of their time unplugged.</p>

<p>The <a href="http://www.29West.Com/" target="_top">29West</a> <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> messaging product may be configured to use only unicast,
thereby allowing development even on an unplugged laptop.</p>

<p>A simpler solution may be to loopback an Ethernet interface so that the kernel sees it
once again as being capable of forwarding multicast between applications. A $5.99
commercial <a href="http://www.computercablestore.com/detail.aspx?ID=3632"
target="_top">product</a> will do the trick.</p>

<p>Note that there may be a time delay between when Ethernet loopback is established and
when the interface becomes usable for multicast loopback. The interface won't be usable
till it has an IP address. It may take a minute or so for a DHCP client to give up on
trying to find a DHCP server. At least on Windows, after DHCP times out, the interface
should get a <a href="http://www.answers.com/topic/link-local-address"
target="_top">169.254.x.y "link local" address</a> if the machine doesn't have a cached
DHCP lease that is still valid.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="SENDING-MULTICAST-MULTIPLE-INTERFACES"
name="SENDING-MULTICAST-MULTIPLE-INTERFACES">10. Sending Multicast on Multiple
Interfaces</a></h2>

<p>It's common for <a href="http://www.29West.Com/" target="_top">29West</a> <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> customers who are concerned about single points of
failure to ask if <b class="APPLICATION">LBM</b> can send one message on two different
interfaces at once. Unfortunately, there is no single thing an application can do through
the socket interface to cause one packet to be sent on two interfaces.</p>

<p>It may seem odd that you can write a single system call that will accept a TCP
connection on all of the interfaces on a machine, yet you can't write a single system
call that will send a packet out on all interfaces. We think this behavior can be traced
back to the presumption that all interfaces will be part of the same internet (or perhaps
the same "Autonomous System" or "<a
href="http://en.wikipedia.org/wiki/Autonomous_system_%28Internet%29"
target="_top">AS</a>" to use BGP parlance). You generally wouldn't want to send the same
packet into your internet more than once. Clearly, this doesn't account for the case
where the interfaces on a machine straddle AS boundaries or cases where you actually
<span class="emphasis"><i class="EMPHASIS">do</i></span> want the same packet sent twice
(say for redundancy).</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="TTL-0-KEEP-MULTICAST-LOCAL"
name="TTL-0-KEEP-MULTICAST-LOCAL">11. TTL=0 to Keep Multicast Local</a></h2>

<p>Some users of <b class="APPLICATION">LBM</b> wish to limit the scope of multicast to
the local machine. This can save some NIC transmit bandwidth for cases where it is known
that there are no interested receivers reachable by network connections.</p>

<p>Setting TTL to 1 is a well-defined way of limiting traffic to the sending LAN, but it
can have undesired consequences (see <a href="#TTL-1-AND-CISCO-CPU-USAGE">Section
12</a>). We haven't yet found a definition for the expected behavior when TTL is set to
0, but limiting the scope to the local machine would seem to be the obvious meaning. We
know that multicast is not copied to local receivers through the loopback interface used
for local unicast receivers. We believe that it may be copied closer to the device driver
level in some kernels. Hence the particular NIC driver in use may have an impact on TTL 0
behavior. It's always best to test for traffic leakage before assuming that TTL 0 will
keep it local to the sending machine.</p>

<p></p>

<p>We have run tests in our <span class="TRADEMARK">Latency Busters</span>&reg; Lab and
found that TTL 0 will keep multicast traffic local on Linux 2.4 kernels, Solaris 8 and
10, Windows XP, AIX, and FreeBSD.</p>

<p></p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="TTL-1-AND-CISCO-CPU-USAGE"
name="TTL-1-AND-CISCO-CPU-USAGE">12. TTL=1 and Cisco CPU Usage</a></h2>

<p>We have seen cases where multicast packets with a TTL of 1 caused high CPU usage on
some Cisco routers. The most direct way to diagnose the problem is to see the symptoms go
away when the TTL is increased beyond the network diameter. We have changed the default
TTL used by our <b class="APPLICATION">LBM</b> product to 16 in the hope of avoiding the
problem. The remainder of this section describes the cause of the problem and suggests
other ways to avoid it.</p>

<p>The problem is known to happen on Cisco Catalyst 65xx and 67xx switches using a
supervisor 720 module. Multicast traffic is normally forwarded at wire speed in hardware
on these switches. However, when a multicast packet arrives for switching with TTL=1, the
hardware passes it up to be process switched instead of forwarding it in hardware.
(Unicast packets with TTL=1 require this behavior so that an ICMP TTL expired message can
be generated in the CPU.)</p>

<p>There are several ways of dealing with the problem outlined in following sections.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="SOLUTION-INCREASE-TTL" name="SOLUTION-INCREASE-TTL">12.1.
Solution: Increase TTL</a></h3>

<p>At the risk of stating the obvious, we have to say that one potential solution is to
configure all multicast sources so that they will always have a TTL larger than the
network diameter and hence will never hit this problem. However, this is best thought of
as voluntary compliance by multicast sources rather than an administrative prohibition. A
common goal is to establish multicast connectivity within one group of networks while
preventing it outside those nets. Source TTL settings alone are often inadequate for
meeting such goals so we present other methods below.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="SOLUTION-IP-MULTICAST-TTL-THRESHOLD"
name="SOLUTION-IP-MULTICAST-TTL-THRESHOLD">12.2. Solution: IP Multicast
TTL-Threshold</a></h3>

<p>The Cisco IOS command <tt class="COMMAND">ip multicast ttl-threshold</tt> looks like
it might help, but it forces all multicast traffic leaving the interface to be routed via
the mcache. This may be better than having the TTL expire and the traffic be process
switched, but <tt class="COMMAND">ip multicast boundary</tt> is probably a better
solution (see <a href="#SOLUTION-IP-MULTICAST-BOUNDARY">Section 12.3</a>).</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="SOLUTION-IP-MULTICAST-BOUNDARY"
name="SOLUTION-IP-MULTICAST-BOUNDARY">12.3. Solution: IP Multicast Boundary</a></h3>

<p>The Cisco IOS command <tt class="COMMAND">ip multicast boundary</tt> can be used to
establish a layer-2 boundary that multicast will not cross. This example configuration
shows how traffic outside the range 239.192.0.0/16 can be prevented from ever passing
passing up to layer-3 processing:</p>

<pre class="SCREEN">
interface Vlan13
 ip multicast boundary 57
. . .
access-list 57 permit 239.192.0.0 0.0.255.255
access-list 57 deny any
</pre>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="SOLUTION-HARDWARE-RATE-LIMIT"
name="SOLUTION-HARDWARE-RATE-LIMIT">12.4. Solution: Hardware Rate Limit</a></h3>

<p>Recent versions (PFC3B or later) of the Supervisor 720 module have hardware that can
be used to limit how often the CPU will be interrupted by packets with expiring TTL. This
example shows how to enable the hardware rate limiter and check its operation:</p>

<pre class="SCREEN">
Router#<kbd class="USERINPUT">mls rate-limit all ttl-failure 500 100</kbd>
Router#<kbd class="USERINPUT">show mls rate-limit</kbd>
  Sharing Codes: S - static, D - dynamic
  Codes dynamic sharing: H - owner (head) of the group, g - guest of the group

    Rate Limiter Type       Status     Packets/s   Burst  Sharing
  ---------------------   ----------   ---------   -----  -------
. . .
            TTL FAILURE   On                 500     100  Not sharing
. . .
Router#
</pre>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="INTERMITTENT-NETWORK-MULTICAST-LOSS"
name="INTERMITTENT-NETWORK-MULTICAST-LOSS">13. Intermittent Network Multicast
Loss</a></h2>

<p>We have had to troubleshoot some tricky cases of intermittent multicast loss in
network equipment while deploying the <a href="http://www.29West.Com/"
target="_top">29West</a> <a href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> messaging product. Such problems can manifest themselves
as spikes in latency while <b class="APPLICATION">LBM</b> repairs the loss (see <a
href="#LATENCY-SOURCES-RETRANSMISSIONS">Section 17.3</a>).</p>

<p>Troubleshooting intermittent loss is a different type of problem than establishing
basic multicast connectivity. Network hardware vendors such as Cisco provide
troubleshooting advice for connectivity problems, but don't seem to offer much advice for
troubleshooting intermittent problems. Notes on our experiences follow.</p>

<p>The root cause of intermittent multicast loss is generally a failing in a mechanism
designed to limit the flow of multicast traffic so that it reaches only interested
receivers. Examples of such mechanisms include IGMP snooping, CGMP, and PIM. Although
they operate in different ways and at different layers, they all depend on receiving
multicast interest information from receivers.</p>

<p>IGMP is used by multicast routers to maintain interest information for hosts on
networks to which they are directly connected. (See the <a
href="http://en.wikipedia.org/wiki/IGMP" target="_top">Wikipedia article on IGMP</a> for
more details.) PIM-SM or another multicast routing protocol must proxy interest
information for receivers reachable only through other multicast routers. Such mechanisms
use timers to clean up the interests of receivers that leave the network without
explicitly revoking their interest. Problems with the settings of these timers can cause
intermittent multicast loss. Similarly, interoperability problems between different
implementations of these mechanisms can cause temporary lapses in multicast
connectivity.</p>

<p>Simple tests can help to diagnose such problems and localize the cause. It's best to
begin by adding multicast receivers at key points in the network topology. A receiver
process added on the same machine as the multicast source can make sure that the source
is at least <span class="emphasis"><i class="EMPHASIS">attempting</i></span> to send the
data. It may also be helpful to add a simple hub or other network tap before the
multicast source reaches the first switch in the network to confirm that packets aren't
being lost before they leave the NIC. Logging multicast traffic with accurate time stamps
and comparing logs across different monitoring points on the network can help isolate the
network component that's causing the intermittent loss.</p>

<p>We used such techniques to identify an IGMP snooping interoperability problem between
3Com, Dell, and Cisco switches on a customer network. The customer chose to work around
the problem by simply disabling IGMP snooping on all the switches. This was a reasonable
choice since the receiver density across switch ports was fairly high and their multicast
rates were quite low so that IGMP snooping wasn't adding much value on their network.</p>

<p>We've also seen a case where an operating system mysteriously stopped answering IGMP
membership queries for interested processes running on the machine. It did continue to
send an initial join message when a process started and a leave message when a process
exited. An IGMP snooping switch heard the initial join message, started forwarding
traffic, but later stopped forwarding it after a timeout had lapsed without hearing
additional membership reports from the port. Rebooting the operating system fixed the
problem.</p>

<p>Similar symptoms may appear if IGMP snooping switches are used without a multicast
router or other source of IGMP membership query messages. Many modern switches that are
capable of IGMP snooping can also act as an IGMP querier. Such a feature must be used if
a LAN doesn't have a multicast router and you want the selective forwarding benefit of
IGMP snooping. If IGMP snooping is enabled without a querier on the LAN, then
intermittent multicast connectivity is likely to result.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MULTICAST-ADDRESS-ASSIGNMENT"
name="MULTICAST-ADDRESS-ASSIGNMENT">14. Multicast Address Assignment</a></h2>

<p>There are several subtle points that often deserve consideration when assigning
multicast addresses. We've collected these as advice and rationale here.</p>

<ul>
<li>
<p><span class="emphasis"><i class="EMPHASIS">Avoid 224.0.0.x</i></span>--Traffic to
addresses of the form 224.0.0.<span class="emphasis"><i class="EMPHASIS">x</i></span> is
often flooded to all switch ports. This address range is reserved for link-local uses.
Many routing protocols assume that all traffic within this range will be received by all
routers on the network. Hence (at least all Cisco) switches flood traffic within this
range. The flooding behavior overrides the normal selective forwarding behavior of a
multicast-aware switch (e.g. IGMP snooping, CGMP, etc.).</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">Watch for 32:1 overlap</i></span>--32
non-contiguous IP multicast addresses are mapped onto each Ethernet multicast address. A
receiver that joins a single IP multicast group implicitly joins 31 others due to this
overlap. Of course, filtering in the operating system discards undesired multicast
traffic from applications, but NIC bandwidth and CPU resources are nonetheless consumed
discarding it. The overlap occurs in the 5 high-order bits, so it's best to use the 23
low-order bits to make distinct multicast streams unique. For example, IP multicast
addresses in the range 239.0.0.0 to 239.127.255.255 all map to unique Ethernet multicast
addresses. However, IP multicast address 239.<span class="emphasis"><i
class="EMPHASIS">128</i></span>.0.0 maps to the same Ethernet multicast address as
239.<span class="emphasis"><i class="EMPHASIS">0</i></span>.0.0, 239.<span
class="emphasis"><i class="EMPHASIS">128</i></span>.0.1 maps to the same Ethernet
multicast address as 239.<span class="emphasis"><i class="EMPHASIS">0</i></span>.0.1,
etc.</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">Avoid x.0.0.y and
x.128.0.y</i></span>--Combining the above two considerations, it's best to avoid using IP
multicast addresses of the form <span class="emphasis"><i
class="EMPHASIS">x</i></span>.0.0.<span class="emphasis"><i class="EMPHASIS">y</i></span>
and <span class="emphasis"><i class="EMPHASIS">x</i></span>.128.0.<span
class="emphasis"><i class="EMPHASIS">y</i></span> since they all map onto the range of
Ethernet multicast addresses that are flooded to all switch ports.</p>
</li>

<li>
<p><span class="emphasis"><i class="EMPHASIS">Watch for address assignment
conflicts</i></span>--<a href="http://www.iana.org/" target="_top">IANA</a> administers
<a href="http://www.iana.org/assignments/multicast-addresses" target="_top">Internet
multicast addresses</a>. Potential conflicts with Internet multicast address assignments
can be avoided by using <a href="http://www.ietf.org/rfc/rfc3180.txt" target="_top">GLOP
addressing</a> ( <a href="http://en.wikipedia.org/wiki/Autonomous_system_%28Internet%29"
target="_top">AS</a> required) or <a href="http://www.ietf.org/rfc/rfc2365.txt"
target="_top">administratively scoped addresses</a>. Such addresses can be safely used on
a network connected to the Internet without fear of conflict with multicast sources
originating on the Internet. Administratively scoped addresses are roughly analogous to
the unicast <a href="http://www.ietf.org/rfc/rfc1918.txt" target="_top">address space for
private internets</a>. Site-local multicast addresses are of the form 239.255.<span
class="emphasis"><i class="EMPHASIS">x</i></span>.<span class="emphasis"><i
class="EMPHASIS">y</i></span>, but can grow down to 239.252.<span class="emphasis"><i
class="EMPHASIS">x</i></span>.<span class="emphasis"><i class="EMPHASIS">y</i></span> if
needed. Organization-local multicast addresses are of the form 239.192-251.<span
class="emphasis"><i class="EMPHASIS">x</i></span>.<span class="emphasis"><i
class="EMPHASIS">y</i></span>, but can grow down to 239.<span class="emphasis"><i
class="EMPHASIS">x</i></span>.<span class="emphasis"><i
class="EMPHASIS">y</i></span>.<span class="emphasis"><i class="EMPHASIS">z</i></span> if
needed.</p>
</li>
</ul>

<p>That's the condensed version of our advice. For a more detailed treatment (57 pages!),
see Cisco's <a
href="http://www.cisco.com/en/US/tech/tk828/technologies_white_paper09186a00802d4643.shtml"
 target="_top">Guidelines for Enterprise IP Multicast Address Allocation</a> paper.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MULTICAST-RETRANSMISSIONS"
name="MULTICAST-RETRANSMISSIONS">15. Multicast Retransmissions</a></h2>

<p>It is quite likely that members of a multicast group will experience different loss
patterns. Group members who experienced loss will be interested in retransmission of the
lost data while members who have already received it will not. In many ways, this
presents conflicts similar to those involved in choosing the best sending rate for the
group (see <a href="#GROUP-RATE-CONTROL">Section 3</a>). The conflicts are particularly
pronounced when loss is experienced by one or a small group of receivers (see <a
href="#CRYBABY-RECEIVER">Section 3.1</a>).</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="MULTICAST-RETRANSMISSIONS-COST-BENEFIT"
name="MULTICAST-RETRANSMISSIONS-COST-BENEFIT">15.1. Retransmission Cost/Benefit</a></h3>

<p>It's important to note how small group loss differs from the case where the packet
loss is experienced by many receivers in the group. When loss is widespread, there is
widespread benefit from the consequent multicast retransmissions. When loss is isolated
to one or a few receivers, they benefit from the retransmissions, but the other receivers
may experience latency while the sender retransmits. In the common case where there is a
limited amount of network bandwidth between the sender and receivers, the bandwidth
needed for retransmissions must be subtracted from that available for new data
transmission. For example, the <a href="http://www.29West.Com/" target="_top">29West</a>
LBT-RM reliable multicast protocol always prioritizes retransmissions ahead of new data,
so they add a small amount of latency when there there is new data waiting to be sent to
all members of the group.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="MULTICAST-RETRANSMISSION-CONTROL"
name="MULTICAST-RETRANSMISSION-CONTROL">15.2. Multicast Retransmission Control
Techniques</a></h3>

<p>Reliable delivery to a multicast group in the face of loss involves a trade off
between throughput/latency and reliable reception for all members. Ideally,
administrative policy should establish the boundaries within which reliability will be
maintained. A simple and effective way to establish a boundary is to limit the amount of
bandwidth that will be used for retransmissions. Establishing such a boundary can
effectively defend a group of receivers against an "attack" from a crybaby receiver. No
matter how much loss is experienced across the receiver set, the sender will limit the
retransmission rate to be within the boundary set by administrative policy.</p>

<p>Note that limiting the retransmission <span class="emphasis"><i
class="EMPHASIS">request</i></span> rate on receivers might be better than doing nothing,
but it's not as effective as limiting the bandwidth available for retransmission. For
example, if a large number of receivers experience loss, then the combined retransmission
request rate could be unacceptably high, even if each individual receiver limits its own
retransmission request rate.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="AVOIDING-MULTICAST-RECEIVER-LOSS"
name="AVOIDING-MULTICAST-RECEIVER-LOSS">15.3. Avoiding Multicast Receiver Loss</a></h3>

<p>Even if retransmission rates have been limited, it is still important to identify the
cause of isolated receiver loss problems and repair them. Usually, such loss is caused by
overrunning NIC buffers or UDP socket buffers. Assuming that the sender cannot be slowed
down, receiver loss can generally avoided by one or more of these means:</p>

<ul>
<li>
<p>Increasing NIC <a href="http://en.wikipedia.org/wiki/Ring_buffer" target="_top">ring
buffer</a> size (e.g. use a brand name, server-class NIC instead of a generic,
workstation-class NIC, also see <a href="#WINDOWS-INTERRUPT-COALESCING">Section
18.3</a>)</p>
</li>

<li>
<p>Decreasing the OS NIC interrupt service latency (e.g. decrease CPU workload or add
more CPUs to a multi-CPU machine)</p>
</li>

<li>
<p>Increasing UDP socket buffer size (see <a
href="#SETTING-KERNEL-UDP-BUFFER-LIMITS">Section 8.8</a>)</p>
</li>

<li>
<p>Decreasing the OS process context switching time (e.g. decrease CPU workload or add
more CPUs to a multi-CPU machine)</p>
</li>
</ul>

<p>Establishing a bandwidth limit on retransmissions will not help an isolated receiver
experiencing loss, but it can be a critical factor in ensuring that one receiver does not
take down the whole group with excessive retransmission requests. Retransmission rate
limits are likely to increase the number of unrecoverable losses on receivers
experiencing loss. Still, it's generally best for the group to first establish a defense
against future crybaby receivers before working to fix any individual receiver
problems.</p>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MESSAGING-LATENCY-BUDGET" name="MESSAGING-LATENCY-BUDGET">16.
Messaging Latency Budget</a></h2>

<p>The latency of a messaging application is the sum of the latencies of its parts.
Often, the primary concern is the application latency, not the latencies of the parts. In
theory, this allows some parts of the application more than average latency for a given
message if other parts use less than average.</p>

<p>In practice, it seldom works out this way. The same factors that make one part take
longer often make other parts take longer. Increasing message size is the obvious
example. Another example is a message traffic burst. It can cause CPU contention thereby
adding latency throughout a messaging application. Network traffic bursts can sharply
increase latency in network queues. See <a
href="#LATENCY-SOURCES-NETWORK-QUEUING">Section 17.8</a> for details.</p>

<p>The challenge is building a messaging application that consistently meets its latency
goals for all workloads. Ideally, failures are logged for later analysis if latency goals
cannot be met. Similar challenges are faced by the builders of VoIP telephony systems and
others concerned with real-time performance. We have seen successful messaging
applications designed by borrowing the concept of a <span class="emphasis"><i
class="EMPHASIS">latency budget</i></span> from other real-time systems.</p>

<p>A latency budget is best applied by first establishing a total latency budget for the
messaging application. Then identify all the sources of latency in the application and
allocate a portion of the application budget to each source. (See <a
href="#LATENCY-SOURCES">Section 17</a> for a list of commonly-encountered latency
sources.) Each latency source is aware of its budget and monitors its performance. It
logs any cases where it can't complete its work within the budgeted time.</p>

<p>Most "managed" pieces of network hardware already do this. Routers and managed
switches have counters that are incremented every time a packet cannot be queued because
a queue is already full. This isn't so much a time budget as it is a queue space budget.
But space can be easily converted to time by dividing by the speed of the interface.</p>

<p>The <a href="http://www.29West.Com/" target="_top">29West</a> <b
class="APPLICATION">LBM</b> messaging system supports latency boundaries in message
batching, latency-bounded TCP, and in our reliable unicast and multicast transport
protocols. Messaging applications are notified whenever loss-free delivery cannot be
maintained within the latency budget.</p>

<p>One of the big benefits we've seen from establishing a latency budget is that it helps
to guide the work and reduce "finger pointing" in large organizations. A messaging
application often can't meet its latency goals without the cooperation of many groups
within a large organization. The team that maintains the network infrastructure must keep
queuing delays to a minimum. The team that administers operating systems must optimize
tuning and limit non-essential load on the OS. The team that budgets for hardware
purchases must make sure adequate CPU and networking hardware is purchased. The team that
administers the messaging system must configure it to make efficient use of available CPU
and network resources. An overall application latency budget that is subdivided over
potential latency sources is a good tool for identifying the root cause of latency
problems when they occur. If each latency source logs cases where it exceeded its budget
or dropped a message, it's much easier to take corrective action.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="LATENCY-SOURCES" name="LATENCY-SOURCES">17. Sources of
Latency</a></h2>

<p>Our work with messaging systems at <a href="http://www.29West.Com/"
target="_top">29West</a> as <span class="TRADEMARK">Latency Busters</span> has given us
many opportunities to investigate sources of latency in messaging. This section lists the
sources we most frequently encounter. It contains links to other sections where latency
sources are discussed in more detail. Sources are listed in approximate order of
decreasing contribution to worst-case system latency. Of course, the amount of latency
due to each source in each system will be different.</p>

<p>Some sources of latency impact every message sent while others impact only some
messages. Some sources have separate fixed and variable components. The fixed component
forms a lower bound on latency for all messages while the variable component contributes
to the variance in latency between messages. Where possible, latency sources will be
characterized by when they occur and their variability.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-INTERMEDIARIES"
name="LATENCY-SOURCES-INTERMEDIARIES">17.1. Intermediaries</a></h3>

<p>Many messaging systems contain components that add little value to the overall system.
The time taken for communication between these components often adds up to a substantial
fraction of the total system latency. Design problems like this are best addressed by
removing or combining components that add little value. <a href="http://www.29West.Com/"
target="_top">29West</a> adopted a "no daemons" design philosophy in its <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> product thereby eliminating latency and design complexity
often found in other leading messaging systems.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-GARBAGE-COLLECTION"
name="LATENCY-SOURCES-GARBAGE-COLLECTION">17.2. Garbage Collection</a></h3>

<p>One of the compelling benefits of managed languages like Java and C# is that
programmers need not worry about tracking reference counts and freeing memory occupied by
unused objects. This pushes the burden of reference counting to the language run-time
code (Java's JVM or C#'s CLR). A common source of intermittent but significant latency
for messaging systems involving managed languages is garbage collection in the language
run-time code.</p>

<p>The latency happens when the run-time systems stops execution of all user code while
counting references. More modern run-time systems allow execution even while garbage
collection is happening (e.g. mark and sweep techniques).</p>

<p>Garbage collection latency generally happens infrequently, but depending on the
system, it can be significant when it does happen. It impacts all messages that arrive
during collection and those that are queued as a result of the latency.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-RETRANSMISSIONS"
name="LATENCY-SOURCES-RETRANSMISSIONS">17.3. Retransmissions</a></h3>

<p>It is common for messaging applications to expect loss-free message delivery even if
the network layer drops packets. This implies that either the messaging layer itself or a
transport layer protocol under it must repair the loss. The time taken to discover the
loss, request retransmission, and the arrival of the retransmission all contribute to
retransmission latency. See <a href="#MULTICAST-RETRANSMISSIONS">Section 15</a> for more
information on multicast retransmissions.</p>

<p>Retransmission latency typically only happens when physical- or network-layer loss
requires retransmission. This is infrequent in well-managed networks. In the event that
retransmission is requested by a receiver detecting loss, the fixed component of
retransmission latency is the RTT from source to receiver while the remainder is
variable.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-REORDERING"
name="LATENCY-SOURCES-REORDERING">17.4. Reordering</a></h3>

<p>Messages may arrive out of order at a receiver due to network loss or simply because
the network has multiple paths allowing some messages faster paths than others. When a
message arrives ahead of some that were sent before it, it can be held until its
predecessors also arrive or may be delivered immediately. The holding process allows
applications to receive messages in order even when they arrive out of order, but it adds
a reordering latency. <a href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> offers an arrival-order delivery feature that avoids this
delay, but TCP offers no such option. See <a href="#TCP-LATENCY">Section 2</a> for a
deeper discussion of reordering latency in TCP.</p>

<p>Note that in the case where a lost message cannot be recovered, the reordering latency
can become very large. Following loss, TCP uses an exponential retransmission algorithm
which can lead to latencies measured in minutes. Bounded-reliability protocols like
LBT-RM and LBT-RU allow control over the timers that detect unrecoverable loss. These
timers can be set to move on much more quickly following unrecoverable loss.</p>

<p>Reordering latency only happens when messages arrive out of order. It is entirely
variable with no fixed component. However, in cases where loss is the cause of reordering
latency, reordering latency is the lesser of the retransmission latency or unrecoverable
loss detection threshold.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-BATCHING"
name="LATENCY-SOURCES-BATCHING">17.5. Batching</a></h3>

<p>Achieving low latency is almost always at odds with efficient use of resources. There
is a fixed overhead associated with every network packet generated and every interrupt
serviced. CPU time is required to generate and decode the packet. Network bandwidth is
required for physical, network, and transport layer protocols. When message payloads are
small relative to the network MTU, this overhead can be amortized over many messages by
batching them together into a single packet. This provides a significant efficiency
improvement over the simple, low-latency alternative of sending one message per
packet.</p>

<p>Similarly, in the world of NIC hardware, a NIC can often be configured to interrupt
the CPU for each network packet as it is received. This provides the lowest possible
latency, but doesn't get very much work done for the fixed cost of servicing the
interrupt. Many modern Gigabit Ethernet NICs have a feature that allows the NIC to delay
interrupting the CPU until several packets have arrived, thus amortizing the fixed cost
of servicing the interrupt over all of the packets serviced. This adds latency in the
interest of efficiency and performance under load. See <a
href="#LATENCY-INTERRUPT-COALESCING">Section 18</a> for more information.</p>

<p>Batching latency may be small or insignificant if messages are sent very quickly. The
most difficult trade offs have to be made when it can't be known in advance when the next
message will be sent. Trade offs may be specified as a maximum latency that would be
added before sending a network packet. Additionally, a minimum size required to trigger
transmission of a network packet could be specified. <b class="APPLICATION">LBM</b>
offers applications control over these batching parameters so that they can optimize the
trade off between efficiency and latency.</p>

<p>Batching latency is variable depending on the batching control parameters and the time
between messages.</p>

<p>Batching can happen in the transport layer as well as in the messaging layer. Nagle's
algorithm is widely used in TCP to improve efficiency by delaying packet transmissions.
See <a href="#TCP-LATENCY-RECOMMENDATIONS">Section 2.4</a> for more information.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-CPU-SCHEDULING"
name="LATENCY-SOURCES-CPU-SCHEDULING">17.6. CPU Scheduling</a></h3>

<p>One of the critical resources a messaging system needs is CPU time. There is often
some latency between when messaging code is ready to run and when it actually gets a CPU
scheduled.</p>

<p>There are generally both fixed and variable components to CPU scheduling latency. The
fixed component is the latency when a CPU is idle and can be immediately scheduled to
messaging code following a device interrupt that makes it ready to run. See <a
href="#MEASURING-CPU-SCHEDULING-LATENCY">Section 20</a> for more background and
measurements we've taken.</p>

<p>The variable component of CPU scheduling latency is often due to contention over CPU
resources. If no idle CPUs are available when messaging code becomes ready to run, then
the CPU scheduling latency will also include CPU contention latency. See <a
href="#MEASURING-CPU-CONTENTION-LATENCY">Section 21</a> for more background and
measurements we've taken.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-SOCKET-BUFFERS"
name="LATENCY-SOURCES-SOCKET-BUFFERS">17.7. Socket Buffers</a></h3>

<p>Socket buffers are present in the OS kernel on both the sending and receiving ends.
These buffers help to smooth out fluctuations in message generation and consumption
rates. However, like any buffering, socket buffers can add latency to a messaging system.
The worst-case latency can be computed by dividing the size of the socket buffer by the
data rate flowing through the buffer.</p>

<p>Socket buffer latency can vary from near zero to the maximum computed above. The
faster messages are produced or the more slowly they are consumed, the more likely they
are to be delayed by socket buffering.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-NETWORK-QUEUING"
name="LATENCY-SOURCES-NETWORK-QUEUING">17.8. Network Queuing</a></h3>

<p>Switches and routers buffer partial or complete packets on queues before forwarding
them. Such buffering helps to smooth out peaks in packet entrance rates that may briefly
exceed the wire speed of an exit interface.</p>

<p>Desktop and consumer-grade switches tend to have little memory for buffering and hence
are generally only willing to queue a packet or two for each exit interface. However,
routers and infrastructure-grade switches have enough memory to buffer several dozen
packets per exit interface. Each queued packet adds latency equal to the time needed to
serialize it (see <a href="#LATENCY-SOURCES-SERIALIZATION">Section 17.10</a>).</p>

<p>Network queuing latency is present whenever switches and routers are used. However, it
is highly variable. On an idle network with cut-through switching, it could be just the
serialization latency for the portion of a packet needed to determine its destination. On
a congested routed network, it could be many dozen times the serialization latency of an
MTU-sized packet.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-NETWORK-ACCESS-CONTROL"
name="LATENCY-SOURCES-NETWORK-ACCESS-CONTROL">17.9. Network Access Control</a></h3>

<p>Sometimes also called network admission control, this latency source is a combination
of factors that may add latency before a packet is sent. The simplest case is where
messages are being sent faster than the wire speed of the network. Latency may also be
added before a packet is sent if there is contention for network bandwidth. (It is a
common misconception that switched networks eliminate such contention. Switches do help,
but the contention point often becomes getting packets out of the switch rather than into
it.) See <a href="#ETHERNET-FLOW-CONTROL">Section 4</a> for a possible cause of network
access control latency.</p>

<p>Transport-layer protocols like LBT-RM may also add latency if an application attempts
to send messages faster than the network can safely deliver them. The rate controls that
impose this latency promise stable network operation in return. Indeed, most forms of
network access control latency end up being beneficial by preventing unfair use or
congestive collapse of the network.</p>

<p>Network access control latency should happen infrequently on modern networks where
message generation rates are matched to the ability of the network to safely carry them.
It can be highly variable and difficult to estimate because it may arise from the actions
of others on the network.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-SERIALIZATION"
name="LATENCY-SOURCES-SERIALIZATION">17.10. Serialization</a></h3>

<p>Networks employ various techniques for serializing data so that it can be moved
conveniently. Typically, a fixed-frequency clock coordinates the action of a sender and
receiver. One bit is often transmitted for each beat of the clock. Serialization latency
is due to the fact that a receiver cannot use a packet until its last bit has
arrived.</p>

<p>The common DS0 communications line operates at 56 Kbps and hence adds 214 ms of
serialization latency to a 1500-byte packet. A DS1 line ("T1") operates at 1.5 Mbps,
adding 8 ms of latency. Even a 100 Mbps Ethernet adds 120 &mu;s of latency on a 1500-byte
packet.</p>

<p>Serialization latency should be constant for a given clock rate. It should be
consistent across all messages.</p>

<div class="NOTE">
<blockquote class="NOTE">
<p><b>Note:</b> Serialization latency and the speed of light (discussed in the next
section) can be easily visualized and contrasted using <a
href="http://media.pearsoncmg.com/aw/aw_kurose_network_2/applets/transmission/delay.html"
target="_top">a Java applet</a>. The linked web page uses the term "transmission delay"
where we use the term "serialization latency." Similarly, it uses the term "propagation
delay" where we use "speed of light."</p>
</blockquote>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-SPEED-OF-LIGHT"
name="LATENCY-SOURCES-SPEED-OF-LIGHT">17.11. Speed of Light</a></h3>

<p>186,000 miles per second--it's not just a good idea, it's the law!</p>

<p>Although often insignificant under one roof or even around a campus, latency due to
the speed of light can be a significant issue in WAN communication. Signals travel
through copper wires and optical fibers at only about 60% of their speed in a vacuum.
Hence the 3,000 km trip from Chicago to San Francisco takes about 15 ms while the 6,400
km trip from Chicago to London takes about 33 ms.</p>

<p>As far as we know, the speed of light is a constant of the universe so we'd expect the
latency it adds over a fixed path to also be constant.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LATENCY-SOURCES-ARP" name="LATENCY-SOURCES-ARP">17.12. Address
Resolution Protocol (ARP)</a></h3>

<p>Address Resolution Protocol, or ARP, is invoked the first time a unicast message is
sent to an IP address.</p>

<ul>
<li>
<p>ARP on the sending host sends a broadcast message to every host on the subnet, asking
"Who has IP x.x.x.x?"</p>
</li>

<li>
<p>The host with IP address "x.x.x.x" responds with its Ethernet address in an ARP
reply.</p>
</li>

<li>
<p>The sending host can now resolve the IP address to an Ethernet address, and send
messages to it.</p>
</li>
</ul>

<p>ARP will cache the Ethernet address for a specific period of time and reuse that
information for future message send operations. After the ARP cache timeout period, the
address is typically dropped from the ARP cache, adding application latency to the next
message sent from that host to that IP address because of the overhead of the address
resolution process. A complicating factor is that different operating systems have
different defaults for this timeout, for example, Linux often uses 1 minute, whereas
Solaris often uses 5 minutes.</p>

<p>Another potential source of latency is an application that is sending unicast messages
very fast while ARP resolution is holding up the sending process. These messages must be
buffered until the ARP reply message is received, which causes latency, and may lead to
loss, adding additional latency for the recovery of those messages.</p>

<p>Note that ARP latency is the total roundtrip cost of the originating host sending the
"Who has..." message and waiting for the response. Users on 1Gbe networks may estimate a
minimum of 80-100 usecs.</p>

<p>All of the above applies to unicast addressing only. With multicast addressing, ARP is
not an issue, because the network router takes care of managing the list of interested
network ports.</p>

<p>One way to eliminate ARP latency completely is to use a static ARP cache, with the
hosts configured to know about each other. Of course, this is a very labor-intensive
solution, and maintaining such configurations can be onerous and error-prone.</p>

<p>Yet another solution is to configure the ARP cache timeout on all hosts on the network
to be much longer, say, 12 hours. This may also require devoting more memory to the ARP
cache itself. If an initial heartbeat message is built into every app to absorb the
latency hit on a meaningless message before the start of the business day, there should
be no ARP-induced latency during business hours. However, one downside to this approach
is if a host's IP address or MAC address were to change during this time, a manual ARP
flush would be required.</p>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="LATENCY-INTERRUPT-COALESCING"
name="LATENCY-INTERRUPT-COALESCING">18. Latency from Interrupt Coalescing</a></h2>

<p>Many <a href="http://www.29West.Com/" target="_top">29West</a> customers using <b
class="APPLICATION">LBM</b> are concerned about latency. We have helped them troubleshoot
latency problems and have sometimes found a significant cause to be interrupt coalescing
in Gigabit Ethernet NIC hardware. Fortunately, the behavior of interrupt coalescing is
configurable and can generally be adjusted to the particular needs of an application.</p>

<p>As mentioned in <a href="#LATENCY-SOURCES-BATCHING">Section 17.5</a>, interrupt
coalescing represents a trade-off between latency and throughput. Coalescing interrupts
always adds latency to arriving messages, but the resulting efficiency gains may be
desirable where high throughput is desired over low latency.</p>

<p>The default for some NICs or drivers is an "adaptive" or "dynamic" interrupt
coalescing setting that seems to significantly favor high throughput over low latency.
The advice in this section is generally aimed toward changing the default to favor low
latency, perhaps at the expense of high throughput.</p>

<p>The details of configuring interrupt coalescing behavior will vary depending on the
operating system and perhaps even the type of NIC in use. We have had specific experience
with Linux using Intel and Broadcom NICs, and with Windows using Intel NICs.</p>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LINUX-INTERRUPT-COALESCING"
name="LINUX-INTERRUPT-COALESCING">18.1. Linux Interrupt Coalescing</a></h3>

<p>On Linux, the conventional way to configure interrupt coalescing seems to be the <tt
class="COMMAND">ethtool</tt> command. However, some NIC drivers seem to require
configuration in other ways. In our experience, if <tt class="COMMAND">ethtool -c
eth0</tt> did not work, then another method was available.</p>

<p></p>

<p>For example, we have seen that with some Intel NICs on Linux, interrupt coalescing is
controlled through the <tt class="COMMAND">modprobe</tt> method of configuring loadable
drivers. <tt class="FILENAME">/etc/modprobe.conf</tt> might look like this:</p>

<pre class="SCREEN">
options e1000 InterruptThrottleRate=8000
</pre>

<p>Some default configurations set <code class="PARAMETER">InterruptThrottleRate</code>
to 1 which selects dynamic interrupt coalescing. We have seen significant reductions in
latency by changing this to a fixed value (e.g. 8,000) as suggested in Intel Application
Note <a href="http://www.intel.com/design/network/applnots/ap450.htm"
target="_top">AP-450</a>. Using a value of 8,000 would limit receive latency to 1
second/8000 = 125 &mu;s worst case.</p>

<p>For NICs and drivers configured with <tt class="COMMAND">ethtool</tt>, make sure the
<code class="PARAMETER">adaptive-rx</code> parameter is <tt class="LITERAL">off</tt> for
the lowest possible latency. Also check the settings of all of the <code
class="PARAMETER">rx-usecs</code> and <code class="PARAMETER">rx-frames</code>
parameters.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="LINUX-ETHTOOL-COMMAND-PARAMETERS"
name="LINUX-ETHTOOL-COMMAND-PARAMETERS">18.2. Linux "ethtool" Command Parameters</a></h3>

<p>The Linux "ethtool -C" command provides a wide array of different types of parameters
that can be configured in various ways to set values related to interrupt coalescing.</p>

<p>Please note that even though "ethtool" provides support for these parameters, the NIC
driver itself may not. Use the "man ethtool" page along with the NIC documentation to
research the exact parameters available in more detail.</p>

<p>The tables below are organized by type of parameter: RX Parameters, TX Parameters, and
Other Parameters.</p>

<p><b class="APPLICATION">RX Parameters</b></p>

<div class="INFORMALTABLE"><a id="AEN780" name="AEN780"></a>
<table border="1" class="CALSTABLE">
<col width="144" />
<col width="432" />
<thead>
<tr>
<th>
<p>Parameter</p>
</th>
<th>
<p>Definition</p>
</th>
</tr>
</thead>

<tbody>
<tr>
<td><tt class="LITERAL">rx-usecs</tt></td>
<td>
<p>Maximum number of microseconds to delay an RX interrupt after receiving a packet. If
0, only rx-max-frames is used. Do not set both rx-usecs and rx-max-frames to 0 as this
would cause no RX interrupts to be generated.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-usecs-low</tt></td>
<td>
<p>Same as <tt class="LITERAL">rx-usecs</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-low</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-usecs-high</tt></td>
<td>
<p>Same as <tt class="LITERAL">rx-usecs</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-high</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-usecs-irq</tt></td>
<td>
<p>Maximum number of microseconds to delay an RX interrupt after receiving a packet while
an IRQ is also being serviced by the host. Some NIC drivers may not support this
feature.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-max-frames</tt></td>
<td>
<p>Maximum number of packets to delay an RX interrupt after receiving a packet. If 0,
only <tt class="LITERAL">rx-usecs</tt> is used. Do not set both <tt
class="LITERAL">rx-usecs</tt> and <tt class="LITERAL">rx-max-frames</tt> to 0 as this
would cause no RX interrupts to be generated.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-max-frames-low</tt></td>
<td>
<p>Same as <tt class="LITERAL">rx-max-frames</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-low</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-max-frames-high</tt></td>
<td>
<p>Same as <tt class="LITERAL">rx-max-frames</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-high</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">rx-max-frames-irq</tt></td>
<td>
<p>Maximum number of packets to delay an RX interrupt after receiving a packet while an
IRQ is also being serviced by the host. Some NIC drivers may not support this
feature.</p>
</td>
</tr>
</tbody>
</table>
</div>

<p><b class="APPLICATION">TX Parameters</b></p>

<div class="INFORMALTABLE"><a id="AEN844" name="AEN844"></a>
<table border="1" class="CALSTABLE">
<col width="144" />
<col width="432" />
<thead>
<tr>
<th>
<p>Parameter</p>
</th>
<th>
<p>Definition</p>
</th>
</tr>
</thead>

<tbody>
<tr>
<td><tt class="LITERAL">tx-usecs</tt></td>
<td>
<p>Maximum number of microseconds to delay a TX interrupt after sending a packet. If 0,
only tx-max-frames is used. Do not set both tx-usecs and tx-max-frames to 0 as this would
cause no TX interrupts to be generated.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-usecs-low</tt></td>
<td>
<p>Same as <tt class="LITERAL">tx-usecs</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-low</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-usecs-high</tt></td>
<td>
<p>Same as <tt class="LITERAL">tx-usecs</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-high</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-usecs-irq</tt></td>
<td>
<p>Maximum number of microseconds to delay a TX interrupt after sending a packet while an
IRQ is also being serviced by the host. Some NICs may not support this feature.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-max-frames</tt></td>
<td>
<p>Maximum number of packets to delay a TX interrupt after sending a packet. If 0, only
tx-usecs is used. Do not set both tx-usecs and tx-max-frames to 0 as this would cause no
TX interrupts to be generated.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-max-frames-low</tt></td>
<td>
<p>Same as <tt class="LITERAL">tx-max-frames</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-low</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-max-frames-high</tt></td>
<td>
<p>Same as <tt class="LITERAL">tx-max-frames</tt>, but used in concert with <tt
class="LITERAL">pkt-rate-high</tt> (see below).</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">tx-max-frames-irq</tt></td>
<td>
<p>Maximum number of packets to delay a TX interrupt after sending a packet while an IRQ
is also being serviced by the host. Some NICs may not support this feature.</p>
</td>
</tr>
</tbody>
</table>
</div>

<p><b class="APPLICATION">Other Parameters</b></p>

<div class="INFORMALTABLE"><a id="AEN905" name="AEN905"></a>
<table border="1" class="CALSTABLE">
<col width="144" />
<col width="432" />
<thead>
<tr>
<th>Parameter</th>
<th>Definition</th>
</tr>
</thead>

<tbody>
<tr>
<td><tt class="LITERAL">adaptive-rx</tt></td>
<td>
<p>An algorithm to improve rx latency under low packet rates and improve throughput under
high packet rates. Some NIC drivers do not support this feature.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">adaptive-tx</tt></td>
<td>
<p>An algorithm to improve tx latency under low packet rates and improve throughput under
high packet rates. Some NIC drivers do not support this feature.</p>
</td>
</tr>

<tr>
<td><tt class="LITERAL">pkt-rate-low</tt></td>
<td>
<p>Rate of packets per second below which a different set of <tt
class="LITERAL">*-usecs</tt> and <tt class="LITERAL">*-max-frames</tt> parameters are
used:</p>

<ul>
<li>
<p><tt class="LITERAL">rx-usecs-low</tt></p>
</li>

<li>
<p><tt class="LITERAL">rx-max-frames-low</tt></p>
</li>

<li>
<p><tt class="LITERAL">tx-usecs-low</tt></p>
</li>

<li>
<p><tt class="LITERAL">tx-max-frames-low</tt></p>
</li>
</ul>

Above this rate, the normal <tt class="LITERAL">*-usecs</tt> and <tt
class="LITERAL">*-max-frames</tt> parameters are used.<br />
<br />
</td>
</tr>

<tr>
<td><tt class="LITERAL">pkt-rate-high</tt></td>
<td>
<p>Rate of packets per second above which a different set of <tt
class="LITERAL">*-usecs</tt> and <tt class="LITERAL">*-max-frames</tt> parameters are
used:</p>

<ul>
<li>
<p><tt class="LITERAL">rx-usecs-high</tt></p>
</li>

<li>
<p><tt class="LITERAL">rx-max-frames-high</tt></p>
</li>

<li>
<p><tt class="LITERAL">tx-usecs-high</tt></p>
</li>

<li>
<p><tt class="LITERAL">tx-max-frames-high</tt></p>
</li>
</ul>

Below this rate, the normal <tt class="LITERAL">*-usecs</tt> and <tt
class="LITERAL">*-max-frames</tt> parameters are used.<br />
<br />
</td>
</tr>

<tr>
<td><tt class="LITERAL">sample-interval</tt></td>
<td>
<p>Number of seconds to use as packet sampling rate for adaptive coalescing. Must be
non-zero.</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="WINDOWS-INTERRUPT-COALESCING"
name="WINDOWS-INTERRUPT-COALESCING">18.3. Windows Interrupt Coalescing</a></h3>

<p>We haven't had as much experience configuring interrupt coalescing on Windows as we
have had on Linux. For a given type of NIC, we'd expect the relevant parameters name to
be similar to those given for Linux above.</p>
</div>

<div class="SECTION">
<hr />
<h3 class="SECTION"><a id="WINDOWS-NIC-LOSS-AVOIDANCE-DETECTION"
name="WINDOWS-NIC-LOSS-AVOIDANCE-DETECTION">18.4. Windows NIC Loss Avoidance and
Detection</a></h3>

<p>We know of a few ways to avoid and detect loss at the NIC level, depending on the
brand of NIC on your Windows machine. Below, we cover <a
href="#WINDOWS-NIC-LOSS-INTEL"><i>Windows and Intel NICs</i></a> and <a
href="#WINDOWS-NIC-LOSS-BROADCOM"><i>Windows and Broadcom NICs</i></a>.</p>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="WINDOWS-NIC-LOSS-INTEL" name="WINDOWS-NIC-LOSS-INTEL">18.4.1.
Windows and Intel NICs</a></h4>

<p>If you have Intel NICs, you might have success doing what one customer did: avoid UDP
loss by increasing the <code class="PARAMETER">Receive Descriptors</code>. It defaulted
to 256 and this customer's NIC model allowed a maximum of 2048. (We recommend that you
use the maximum <code class="PARAMETER">Receive Descriptors</code> value for your NIC
model whatever it is.) See <a href="#RECEIVEDESCRIPTORS">Figure 3</a>.</p>

<div class="FIGURE"><a id="RECEIVEDESCRIPTORS" name="RECEIVEDESCRIPTORS"></a>
<p><b>Figure 3. Windows Intel NIC Receive Descriptors Parameter Setting</b></p>

<p><img src="ReceiveDescriptors.png" align="CENTER" /></p>
</div>

<p>Increasing the <code class="PARAMETER">Receive Descriptors</code> parameter doesn't
change interrupt coalescing settings. It simply increases the size of the <a
href="http://en.wikipedia.org/wiki/Ring_buffer" target="_top">ring buffer</a> the NIC
uses for receiving. This allows for more interrupt servicing latency before loss.</p>

<p></p>

<div class="NOTE">
<blockquote class="NOTE">
<p><b>Note:</b> As shown in the screen shot, each <code class="PARAMETER">Receive
Descriptor</code> requires 2 KB of memory. Our customer's increase to 2048 means that
each Intel NIC would allocate 2 MB of <span class="emphasis"><i
class="EMPHASIS">physical</i></span> memory for its receive ring buffer.</p>

<p></p>
</blockquote>
</div>

<br />
<br />
</div>

<div class="SECTION">
<hr />
<h4 class="SECTION"><a id="WINDOWS-NIC-LOSS-BROADCOM"
name="WINDOWS-NIC-LOSS-BROADCOM">18.4.2. Windows and Broadcom NICs</a></h4>

<p>With Broadcom NICs, we recommend that you download and install the BACS tool to detect
loss, and possibly to adjust tuning options on the NIC to help address it.</p>

<p>First, download BACS (Broadcom Advanced Control Suites) from the install CD that came
with your NIC, or your hardware vendor. You might find this <a
href="http://www.broadcom.com/support/ethernet_nic/faq_drivers.php"
target="_top">Broadcom NICs FAQ page</a> useful.</p>

<p>After downloading and installing it, start BACS. Then run a workload on your machine
that will drive the network traffic you are interested in diagnosing. Then click
Start--&#62;Control Panel, and look in the list for Broadcom Control Suite. Double click
on it to open the Broadcom Advanced Control Suite dialog.</p>

<p>In this dialog, first check the list of Network Interfaces and make sure the Broadcom
NIC is highlighted. Then, over on the upper right, click the Statistics tab, and scroll
down the list of statistics to a statistic called "Out of Recv. Buffer". This value is
the number of packets dropped due to a shortage of buffer space for the NIC. If the value
is nonzero, you may wish to increase the size of your buffers.</p>
</div>
</div>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="LATENCY-MEASUREMENT-OVERVIEW"
name="LATENCY-MEASUREMENT-OVERVIEW">19. Latency Measurement Overview</a></h2>

<p>In our work at <a href="http://www.29West.Com/" target="_top">29West</a>, we wanted to
measure the latency involved in delivering messages through our <a
href="http://www.29West.Com/products/lbm/" target="_top"><b
class="APPLICATION">LBM</b></a> product. Latency in messaging systems with <b
class="APPLICATION">LBM</b> must often be measured in microseconds because
sub-millisecond results are common. The extremely low latency of <b
class="APPLICATION">LBM</b> raises measurement issues not often found with leading
messaging systems. Following sections discuss some of these issues and provide results
from some of our latency measurements. See <a href="#LATENCY-SOURCES">Section 17</a> for
background on the latency sources we've noted during testing in our <span
class="TRADEMARK">Latency Busters</span> Lab.</p>

<p>One of the largest sources of latency we saw in our tests was network loss repair.
Time is required for the transport layer to first detect the loss, request
retransmission, and finally for the repair to arrive. See <a
href="#LATENCY-SOURCES-RETRANSMISSIONS">Section 17.3</a> for a general discussion of
retransmissions and latency. See <a href="#TCP-LATENCY">Section 2</a> for a discussion of
such latency in TCP. For reliable multicast transport protocols such as 29West's LBT-RM,
minimum loss repair latency comes at a cost of efficiency. Systems using reliable
multicast protocols for their scalability often have to trade off efficiency to get the
lowest possible loss repair latency.</p>

<p>No matter what transport protocol is in use, it's important to check for loss at the
first hint of latency. See the end of <a href="#TCP-RECEIVER-SIDE-LATENCY">Section
2.2</a> for some advice on diagnosing TCP loss. See <a
href="#AVOIDING-MULTICAST-RECEIVER-LOSS">Section 15.3</a> for advice on how to avoid loss
when using multicast.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MEASURING-CPU-SCHEDULING-LATENCY"
name="MEASURING-CPU-SCHEDULING-LATENCY">20. Measuring CPU Scheduling Latency</a></h2>

<p>Once latency due to repair loss has been removed from a messaging system, the next
largest source is often CPU scheduling latency. This happens when a process is ready to
run, but is unable to get a CPU on which to run. There is usually both a fixed and
variable component in CPU scheduling latency. The remainder of this section describes
tests we've run to better understand latency in messaging systems.</p>

<p>We ran a series of tests in our <span class="TRADEMARK">Latency Busters</span> Lab
using Solaris 2.8 on a pair of SPARC machines. One machine ran 5 <b
class="APPLICATION">LBM</b> sources, each producing a message stream of 2,000-byte
messages at the rate of 10 messages per second. The other machine ran 10 <b
class="APPLICATION">LBM</b> receivers, each consuming all the messages from each of the 5
sources. Hence the aggregate payload rate across all sources was 100,000 BPS and the
aggregate payload rate across all receivers was 1,000,000 BPS. Our LBT-RM reliable
multicast protocol was used so that the network bandwidth between machines was only
100,000 BPS plus protocol overhead.</p>

<p>We chose a relatively low message rate with a modest size payload for these tests to
emphasize latency over throughput. Note that the 2,000-byte message payload together with
the 1500-byte MTU of Ethernet force <b class="APPLICATION">LBM</b> to break every message
into two Ethernet packets. This magnifies the effect of network loss on latency because
the loss of either of the two packets would delay delivery.</p>

<p>Our first test run was meant to establish a baseline for further tests. We were
particularly interested in the maximum latency measured over a long-running (20 minute)
test. We measured a maximum latency of about 86 ms even though the average was only 1.8
ms. The plot below shows the latency measured for each of the 600,000 messages received
for this test.</p>

<div class="FIGURE"><a id="NORMALUSERLATENCY" name="NORMALUSERLATENCY"></a>
<p><b>Figure 4. Baseline Latency Measurement</b></p>

<p><img src="NormalUserLatency.png" align="CENTER" /></p>
</div>

<p>Only 1 in 1,000 messages had latency over 5.6 ms., however the above plot makes it
apparent that many messages had much higher latencies.</p>

<p>Working on the assumption that CPU scheduling latency might be a significant factor in
the large latency variance observed in the baseline, we tried increasing the CPU
scheduling priority of the <b class="APPLICATION">LBM</b> processes. We used the command
prefix <tt class="COMMAND">nice --19</tt> to elevate the priority of all <b
class="APPLICATION">LBM</b> source and receiver processes to the maximum allowable under
the Solaris "time sharing" scheduling class. Surprisingly, this had little effect on
latency. The maximum measured latency remained unchanged. The plot below shows this.</p>

<div class="FIGURE"><a id="NICE-19LATENCY" name="NICE-19LATENCY"></a>
<p><b>Figure 5. Latency with Maximum Time Sharing Priority</b></p>

<p><img src="Nice-19Latency.png" align="CENTER" /></p>
</div>

<p>The lack of a significant change lead us to conclude that if CPU scheduling latency
was the cause of the large maximum latency, then it wasn't due to CPU contention with
other user time sharing processes on the machine.</p>

<p>This led us to try a CPU scheduling class offered by Solaris called "real time." We
used the command prefix <tt class="COMMAND">priocntl -c RT -e</tt> to request real time
scheduling for all <b class="APPLICATION">LBM</b> processes. This had a dramatic effect
on the maximum latency, reducing it over 16 fold to just 5.2 ms. The plot below shows
this.</p>

<div class="FIGURE"><a id="RTLATENCY" name="RTLATENCY"></a>
<p><b>Figure 6. Latency with Real Time Priority</b></p>

<p><img src="RTLatency.png" align="CENTER" /></p>
</div>

<p>The seemingly random incidence of high-latency messages completely disappeared when <b
class="APPLICATION">LBM</b> ran with real time priority. We conclude that CPU scheduling
latency was the primary cause of the large maximum latency we saw in earlier test.
However, it seems that the source of CPU contention was something within the Solaris
kernel since maximum user CPU priority produced no reduction in latency.</p>
</div>

<div class="SECTION">
<hr />
<h2 class="SECTION"><a id="MEASURING-CPU-CONTENTION-LATENCY"
name="MEASURING-CPU-CONTENTION-LATENCY">21. Measuring CPU Contention Latency</a></h2>

<p>A special form of CPU scheduling latency called CPU contention latency happens when
there are more runnable processes than CPUs. In effect, this separates the fixed portion
of CPU scheduling latency from the variable portion. In our final round of tests, we
wanted to investigate the effect of CPU contention on latency. In the preceding tests, 10
receiver processes had to contend for time on 4 physical CPUs. Note that a single
multicast packet wakes up all 10 receiver process at the same time. Assuming an otherwise
idle machine, 4 processes get scheduled onto CPUs immediately while 6 wait.</p>

<p>For this test, we ran only 3 receiver processes on the same 4-CPU machine. The results
are shown in the lower line on the plot below. The upper line shows the usual 10 receiver
case from above. But note that the vertical axis covers only about 1/7 the latency range
of the preceding plots. The range of the vertical scale was reduced to show more
detail.</p>

<div class="FIGURE"><a id="LBMLATENCY" name="LBMLATENCY"></a>
<p><b>Figure 7. Effect of CPU Contention on Scheduling Latency</b></p>

<p><img src="LBMLatency.png" align="CENTER" /></p>
</div>

<p>The lower band is much thinner than the upper one since there were fewer receivers
than CPUs and hence no CPU contention. The difference in thickness is a good indication
of how long the additional receivers in the earlier tests had to wait for a CPU to run
them (the CPU contention latency). From inspection of the plot above, we'd estimate the
CPU contention latency for the 10 receiver case to be about 1.5 ms.</p>

<p>With the magnified vertical scale of the above plot, the clock drift over the duration
of the test is also apparent. This makes it easy to visualize the operation of <tt
class="COMMAND">ntpd</tt> in trying to keep the system clocks synchronized. Note that the
approximately 7 ms. of clock drift during the test run significantly exceeds the measured
latency for almost all of the test. Also note that the clock skew was negative for the
entire 20-minute duration of the test.</p>
</div>
</div>

<hr />
<p align="center">Copyright 2004 - 2010 29West, Inc.</p>
</body>
</html>

